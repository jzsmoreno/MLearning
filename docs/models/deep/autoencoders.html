<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.5">
<title>likelihood.models.deep.autoencoders API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>likelihood.models.deep.autoencoders</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="likelihood.models.deep.autoencoders.build_model"><code class="name flex">
<span>def <span class="ident">build_model</span></span>(<span>hp, input_shape_parm: None | int, num_classes: None | int, **kwargs) ‑> <a title="likelihood.models.deep.autoencoders.AutoClassifier" href="#likelihood.models.deep.autoencoders.AutoClassifier">AutoClassifier</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_model(
    hp, input_shape_parm: None | int, num_classes: None | int, **kwargs
) -&gt; AutoClassifier:
    &#34;&#34;&#34;Builds a neural network model using Keras Tuner&#39;s search algorithm.

    Parameters
    ----------
    hp : `keras_tuner.HyperParameters`
        The hyperparameters to tune.
    input_shape_parm : `None` | `int`
        The shape of the input data.
    num_classes : `int`
        The number of classes in the dataset.

    Keyword Arguments:
    ----------
    Additional keyword arguments to pass to the model.

    hyperparameters : `dict`
        The hyperparameters to set.

    Returns
    -------
    `keras.Model`
        The neural network model.
    &#34;&#34;&#34;
    hyperparameters = kwargs.get(&#34;hyperparameters&#34;, None)
    hyperparameters_keys = hyperparameters.keys() if hyperparameters is not None else []

    units = (
        hp.Int(
            &#34;units&#34;,
            min_value=int(input_shape_parm * 0.2),
            max_value=int(input_shape_parm * 1.5),
            step=2,
        )
        if &#34;units&#34; not in hyperparameters_keys
        else hyperparameters[&#34;units&#34;]
    )
    activation = (
        hp.Choice(&#34;activation&#34;, [&#34;sigmoid&#34;, &#34;relu&#34;, &#34;tanh&#34;, &#34;selu&#34;, &#34;softplus&#34;, &#34;softsign&#34;])
        if &#34;activation&#34; not in hyperparameters_keys
        else hyperparameters[&#34;activation&#34;]
    )
    optimizer = (
        hp.Choice(&#34;optimizer&#34;, [&#34;sgd&#34;, &#34;adam&#34;, &#34;adadelta&#34;, &#34;rmsprop&#34;, &#34;adamax&#34;, &#34;adagrad&#34;])
        if &#34;optimizer&#34; not in hyperparameters_keys
        else hyperparameters[&#34;optimizer&#34;]
    )
    threshold = (
        hp.Float(&#34;threshold&#34;, min_value=0.1, max_value=0.9, sampling=&#34;log&#34;)
        if &#34;threshold&#34; not in hyperparameters_keys
        else hyperparameters[&#34;threshold&#34;]
    )
    num_layers = (
        hp.Int(&#34;num_layers&#34;, min_value=1, max_value=10, step=1)
        if &#34;num_layers&#34; not in hyperparameters_keys
        else hyperparameters[&#34;num_layers&#34;]
    )

    model = call_existing_code(
        units=units,
        activation=activation,
        threshold=threshold,
        optimizer=optimizer,
        input_shape_parm=input_shape_parm,
        num_classes=num_classes,
        num_layers=num_layers,
    )
    return model</code></pre>
</details>
<div class="desc"><p>Builds a neural network model using Keras Tuner's search algorithm.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>hp</code></strong> :&ensp;<code>keras_tuner.HyperParameters</code></dt>
<dd>The hyperparameters to tune.</dd>
<dt><strong><code>input_shape_parm</code></strong> :&ensp;<code>None` | `int</code></dt>
<dd>The shape of the input data.</dd>
<dt><strong><code>num_classes</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of classes in the dataset.</dd>
</dl>
<h2 id="keyword-arguments">Keyword Arguments:</h2>
<p>Additional keyword arguments to pass to the model.</p>
<p>hyperparameters : <code>dict</code>
The hyperparameters to set.</p>
<h2 id="returns">Returns</h2>
<p><code>keras.Model</code>
The neural network model.</p></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.call_existing_code"><code class="name flex">
<span>def <span class="ident">call_existing_code</span></span>(<span>units: int,<br>activation: str,<br>threshold: float,<br>optimizer: str,<br>input_shape_parm: None | int = None,<br>num_classes: None | int = None,<br>num_layers: int = 1) ‑> <a title="likelihood.models.deep.autoencoders.AutoClassifier" href="#likelihood.models.deep.autoencoders.AutoClassifier">AutoClassifier</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call_existing_code(
    units: int,
    activation: str,
    threshold: float,
    optimizer: str,
    input_shape_parm: None | int = None,
    num_classes: None | int = None,
    num_layers: int = 1,
) -&gt; AutoClassifier:
    &#34;&#34;&#34;
    Calls an existing AutoClassifier instance.

    Parameters
    ----------
    units : `int`
        The number of neurons in each hidden layer.
    activation : `str`
        The type of activation function to use for the neural network layers.
    threshold : `float`
        The threshold for the classifier.
    optimizer : `str`
        The type of optimizer to use for the neural network layers.
    input_shape_parm : `None` | `int`
        The shape of the input data.
    num_classes : `int`
        The number of classes in the dataset.

    Returns
    -------
    `AutoClassifier`
        The AutoClassifier instance.
    &#34;&#34;&#34;
    model = AutoClassifier(
        input_shape_parm=input_shape_parm,
        num_classes=num_classes,
        units=units,
        activation=activation,
        num_layers=num_layers,
    )
    model.compile(
        optimizer=optimizer,
        loss=tf.keras.losses.CategoricalCrossentropy(),
        metrics=[tf.keras.metrics.F1Score(threshold=threshold)],
    )
    return model</code></pre>
</details>
<div class="desc"><p>Calls an existing AutoClassifier instance.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>units</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of neurons in each hidden layer.</dd>
<dt><strong><code>activation</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of activation function to use for the neural network layers.</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>The threshold for the classifier.</dd>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of optimizer to use for the neural network layers.</dd>
<dt><strong><code>input_shape_parm</code></strong> :&ensp;<code>None` | `int</code></dt>
<dd>The shape of the input data.</dd>
<dt><strong><code>num_classes</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of classes in the dataset.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="likelihood.models.deep.autoencoders.AutoClassifier" href="#likelihood.models.deep.autoencoders.AutoClassifier">AutoClassifier</a></code>
The AutoClassifier instance.</p></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.setup_model"><code class="name flex">
<span>def <span class="ident">setup_model</span></span>(<span>data: pandas.core.frame.DataFrame,<br>target: str,<br>epochs: int,<br>train_size: float = 0.7,<br>seed=None,<br>train_mode: bool = True,<br>filepath: str = './my_dir/best_model',<br>method: str = 'Hyperband',<br>**kwargs) ‑> <a title="likelihood.models.deep.autoencoders.AutoClassifier" href="#likelihood.models.deep.autoencoders.AutoClassifier">AutoClassifier</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@suppress_warnings
def setup_model(
    data: DataFrame,
    target: str,
    epochs: int,
    train_size: float = 0.7,
    seed=None,
    train_mode: bool = True,
    filepath: str = &#34;./my_dir/best_model&#34;,
    method: str = &#34;Hyperband&#34;,
    **kwargs,
) -&gt; AutoClassifier:
    &#34;&#34;&#34;Setup model for training and tuning.

    Parameters
    ----------
    data : `DataFrame`
        The dataset to train the model on.
    target : `str`
        The name of the target column.
    epochs : `int`
        The number of epochs to train the model for.
    train_size : `float`
        The proportion of the dataset to use for training.
    seed : `Any` | `int`
        The random seed to use for reproducibility.
    train_mode : `bool`
        Whether to train the model or not.
    filepath : `str`
        The path to save the best model to.
    method : `str`
        The method to use for hyperparameter tuning. Options are &#34;Hyperband&#34; and &#34;RandomSearch&#34;.

    Keyword Arguments:
    ----------
    Additional keyword arguments to pass to the model.

    max_trials : `int`
        The maximum number of trials to perform.
    directory : `str`
        The directory to save the model to.
    project_name : `str`
        The name of the project.
    objective : `str`
        The objective to optimize.
    verbose : `bool`
        Whether to print verbose output.
    hyperparameters : `dict`
        The hyperparameters to set.

    Returns
    -------
    model : `AutoClassifier`
        The trained model.
    &#34;&#34;&#34;
    max_trials = kwargs.get(&#34;max_trials&#34;, 10)
    directory = kwargs.get(&#34;directory&#34;, &#34;./my_dir&#34;)
    project_name = kwargs.get(&#34;project_name&#34;, &#34;get_best&#34;)
    objective = kwargs.get(&#34;objective&#34;, &#34;val_loss&#34;)
    verbose = kwargs.get(&#34;verbose&#34;, True)
    hyperparameters = kwargs.get(&#34;hyperparameters&#34;, None)

    X = data.drop(columns=target)
    input_sample = X.sample(1)
    y = data[target]
    assert (
        X.select_dtypes(include=[&#34;object&#34;]).empty == True
    ), &#34;Categorical variables within the DataFrame must be encoded, this is done by using the DataFrameEncoder from likelihood.&#34;
    validation_split = 1.0 - train_size

    if train_mode:
        try:
            if (not os.path.exists(directory)) and directory != &#34;./&#34;:
                os.makedirs(directory)
            elif directory != &#34;./&#34;:
                print(f&#34;Directory {directory} already exists, it will be deleted.&#34;)
                rmtree(directory)
                os.makedirs(directory)
        except:
            print(&#34;Warning: unable to create directory&#34;)

        y_encoder = OneHotEncoder()
        y = y_encoder.encode(y.to_list())
        X = X.to_numpy()
        input_sample.to_numpy()
        X = np.asarray(X).astype(np.float32)
        input_sample = np.asarray(input_sample).astype(np.float32)
        y = np.asarray(y).astype(np.float32)

        input_shape_parm = X.shape[1]
        num_classes = y.shape[1]
        global build_model
        build_model = partial(
            build_model,
            input_shape_parm=input_shape_parm,
            num_classes=num_classes,
            hyperparameters=hyperparameters,
        )

        if method == &#34;Hyperband&#34;:
            tuner = keras_tuner.Hyperband(
                hypermodel=build_model,
                objective=objective,
                max_epochs=epochs,
                factor=3,
                directory=directory,
                project_name=project_name,
                seed=seed,
            )
        elif method == &#34;RandomSearch&#34;:
            tuner = keras_tuner.RandomSearch(
                hypermodel=build_model,
                objective=objective,
                max_trials=max_trials,
                directory=directory,
                project_name=project_name,
                seed=seed,
            )

        tuner.search(X, y, epochs=epochs, validation_split=validation_split, verbose=verbose)
        models = tuner.get_best_models(num_models=2)
        best_model = models[0]
        best_model(input_sample)

        best_model.save(filepath, save_format=&#34;tf&#34;)

        if verbose:
            tuner.results_summary()
    else:
        best_model = tf.keras.models.load_model(filepath)

    best_hps = tuner.get_best_hyperparameters(1)[0].values
    return best_model, pd.DataFrame(best_hps, index=[&#34;Value&#34;])</code></pre>
</details>
<div class="desc"><p>Setup model for training and tuning.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>The dataset to train the model on.</dd>
<dt><strong><code>target</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the target column.</dd>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of epochs to train the model for.</dd>
<dt><strong><code>train_size</code></strong> :&ensp;<code>float</code></dt>
<dd>The proportion of the dataset to use for training.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>Any` | `int</code></dt>
<dd>The random seed to use for reproducibility.</dd>
<dt><strong><code>train_mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to train the model or not.</dd>
<dt><strong><code>filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to save the best model to.</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>str</code></dt>
<dd>The method to use for hyperparameter tuning. Options are "Hyperband" and "RandomSearch".</dd>
</dl>
<h2 id="keyword-arguments">Keyword Arguments:</h2>
<p>Additional keyword arguments to pass to the model.</p>
<p>max_trials : <code>int</code>
The maximum number of trials to perform.
directory : <code>str</code>
The directory to save the model to.
project_name : <code>str</code>
The name of the project.
objective : <code>str</code>
The objective to optimize.
verbose : <code>bool</code>
Whether to print verbose output.
hyperparameters : <code>dict</code>
The hyperparameters to set.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code><a title="likelihood.models.deep.autoencoders.AutoClassifier" href="#likelihood.models.deep.autoencoders.AutoClassifier">AutoClassifier</a></code></dt>
<dd>The trained model.</dd>
</dl></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.suppress_warnings"><code class="name flex">
<span>def <span class="ident">suppress_warnings</span></span>(<span>func)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def suppress_warnings(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        with warnings.catch_warnings():
            warnings.simplefilter(&#34;ignore&#34;)
            return func(*args, **kwargs)

    return wrapper</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="likelihood.models.deep.autoencoders.AutoClassifier"><code class="flex name class">
<span>class <span class="ident">AutoClassifier</span></span>
<span>(</span><span>input_shape_parm, num_classes, units, activation, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@tf.keras.utils.register_keras_serializable(package=&#34;Custom&#34;, name=&#34;AutoClassifier&#34;)
class AutoClassifier(tf.keras.Model):
    &#34;&#34;&#34;
    An auto-classifier model that automatically determines the best classification strategy based on the input data.

    Attributes:
        - input_shape_parm: The shape of the input data.
        - num_classes: The number of classes in the dataset.
        - units: The number of neurons in each hidden layer.
        - activation: The type of activation function to use for the neural network layers.

    Methods:
        __init__(self, input_shape_parm, num_classes, units, activation): Initializes an AutoClassifier instance with the given parameters.
        build(self, input_shape_parm): Builds the model architecture based on input_shape_parm.
        call(self, x): Defines the forward pass of the model.
        get_config(self): Returns the configuration of the model.
        from_config(cls, config): Recreates an instance of AutoClassifier from its configuration.
    &#34;&#34;&#34;

    def __init__(self, input_shape_parm, num_classes, units, activation, **kwargs):
        &#34;&#34;&#34;
        Initializes an AutoClassifier instance with the given parameters.

        Parameters
        ----------
        input_shape_parm : `int`
            The shape of the input data.
        num_classes : `int`
            The number of classes in the dataset.
        units : `int`
            The number of neurons in each hidden layer.
        activation : `str`
            The type of activation function to use for the neural network layers.

        Keyword Arguments:
        ----------
        Additional keyword arguments to pass to the model.

        classifier_activation : `str`
            The activation function to use for the classifier layer. Default is &#34;softmax&#34;. If the activation function is not a classification function, the model can be used in regression problems.
        num_layers : `int`
            The number of hidden layers in the classifier. Default is 1.
        &#34;&#34;&#34;
        super(AutoClassifier, self).__init__()
        self.input_shape_parm = input_shape_parm
        self.num_classes = num_classes
        self.units = units
        self.activation = activation

        self.encoder = None
        self.decoder = None
        self.classifier = None
        self.classifier_activation = kwargs.get(&#34;classifier_activation&#34;, &#34;softmax&#34;)
        self.num_layers = kwargs.get(&#34;num_layers&#34;, 1)

    def build(self, input_shape):
        self.encoder = tf.keras.Sequential(
            [
                tf.keras.layers.Dense(units=self.units, activation=self.activation),
                tf.keras.layers.Dense(units=int(self.units / 2), activation=self.activation),
            ]
        )

        self.decoder = tf.keras.Sequential(
            [
                tf.keras.layers.Dense(units=self.units, activation=self.activation),
                tf.keras.layers.Dense(units=self.input_shape_parm, activation=self.activation),
            ]
        )

        self.classifier = tf.keras.Sequential()
        if self.num_layers &gt; 1:
            for _ in range(self.num_layers - 1):
                self.classifier.add(
                    tf.keras.layers.Dense(units=self.units, activation=self.activation)
                )
        self.classifier.add(
            tf.keras.layers.Dense(units=self.num_classes, activation=self.classifier_activation)
        )

    def call(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        combined = tf.concat([decoded, encoded], axis=1)
        classification = self.classifier(combined)
        return classification

    def get_config(self):
        config = {
            &#34;input_shape_parm&#34;: self.input_shape_parm,
            &#34;num_classes&#34;: self.num_classes,
            &#34;units&#34;: self.units,
            &#34;activation&#34;: self.activation,
            &#34;classifier_activation&#34;: self.classifier_activation,
            &#34;num_layers&#34;: self.num_layers,
        }
        base_config = super(AutoClassifier, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    @classmethod
    def from_config(cls, config):
        return cls(
            input_shape_parm=config[&#34;input_shape_parm&#34;],
            num_classes=config[&#34;num_classes&#34;],
            units=config[&#34;units&#34;],
            activation=config[&#34;activation&#34;],
            classifier_activation=config[&#34;classifier_activation&#34;],
            num_layers=config[&#34;num_layers&#34;],
        )</code></pre>
</details>
<div class="desc"><p>An auto-classifier model that automatically determines the best classification strategy based on the input data.</p>
<h2 id="attributes">Attributes</h2>
<ul>
<li>input_shape_parm: The shape of the input data.</li>
<li>num_classes: The number of classes in the dataset.</li>
<li>units: The number of neurons in each hidden layer.</li>
<li>activation: The type of activation function to use for the neural network layers.</li>
</ul>
<h2 id="methods">Methods</h2>
<p><strong>init</strong>(self, input_shape_parm, num_classes, units, activation): Initializes an AutoClassifier instance with the given parameters.
build(self, input_shape_parm): Builds the model architecture based on input_shape_parm.
call(self, x): Defines the forward pass of the model.
get_config(self): Returns the configuration of the model.
from_config(cls, config): Recreates an instance of AutoClassifier from its configuration.</p>
<p>Initializes an AutoClassifier instance with the given parameters.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_shape_parm</code></strong> :&ensp;<code>int</code></dt>
<dd>The shape of the input data.</dd>
<dt><strong><code>num_classes</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of classes in the dataset.</dd>
<dt><strong><code>units</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of neurons in each hidden layer.</dd>
<dt><strong><code>activation</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of activation function to use for the neural network layers.</dd>
</dl>
<h2 id="keyword-arguments">Keyword Arguments:</h2>
<p>Additional keyword arguments to pass to the model.</p>
<p>classifier_activation : <code>str</code>
The activation function to use for the classifier layer. Default is "softmax". If the activation function is not a classification function, the model can be used in regression problems.
num_layers : <code>int</code>
The number of hidden layers in the classifier. Default is 1.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.src.models.model.Model</li>
<li>keras.src.backend.tensorflow.trainer.TensorFlowTrainer</li>
<li>keras.src.trainers.trainer.Trainer</li>
<li>keras.src.layers.layer.Layer</li>
<li>keras.src.backend.tensorflow.layer.TFLayer</li>
<li>keras.src.backend.tensorflow.trackable.KerasAutoTrackable</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.src.ops.operation.Operation</li>
<li>keras.src.saving.keras_saveable.KerasSaveable</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="likelihood.models.deep.autoencoders.AutoClassifier.from_config"><code class="name flex">
<span>def <span class="ident">from_config</span></span>(<span>config)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an operation from its config.</p>
<p>This method is the reverse of <code>get_config</code>, capable of instantiating the
same operation from the config dictionary.</p>
<p>Note: If you override this method, you might receive a serialized dtype
config, which is a <code>dict</code>. You can deserialize it as follows:</p>
<pre><code class="language-python">if &quot;dtype&quot; in config and isinstance(config[&quot;dtype&quot;], dict):
    policy = dtype_policies.deserialize(config[&quot;dtype&quot;])
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config</code></strong></dt>
<dd>A Python dictionary, typically the output of <code>get_config</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>An operation instance.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.models.deep.autoencoders.AutoClassifier.build"><code class="name flex">
<span>def <span class="ident">build</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build(self, input_shape):
    self.encoder = tf.keras.Sequential(
        [
            tf.keras.layers.Dense(units=self.units, activation=self.activation),
            tf.keras.layers.Dense(units=int(self.units / 2), activation=self.activation),
        ]
    )

    self.decoder = tf.keras.Sequential(
        [
            tf.keras.layers.Dense(units=self.units, activation=self.activation),
            tf.keras.layers.Dense(units=self.input_shape_parm, activation=self.activation),
        ]
    )

    self.classifier = tf.keras.Sequential()
    if self.num_layers &gt; 1:
        for _ in range(self.num_layers - 1):
            self.classifier.add(
                tf.keras.layers.Dense(units=self.units, activation=self.activation)
            )
    self.classifier.add(
        tf.keras.layers.Dense(units=self.num_classes, activation=self.classifier_activation)
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.AutoClassifier.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, x):
    encoded = self.encoder(x)
    decoded = self.decoder(encoded)
    combined = tf.concat([decoded, encoded], axis=1)
    classification = self.classifier(combined)
    return classification</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.AutoClassifier.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = {
        &#34;input_shape_parm&#34;: self.input_shape_parm,
        &#34;num_classes&#34;: self.num_classes,
        &#34;units&#34;: self.units,
        &#34;activation&#34;: self.activation,
        &#34;classifier_activation&#34;: self.classifier_activation,
        &#34;num_layers&#34;: self.num_layers,
    }
    base_config = super(AutoClassifier, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
<div class="desc"><p>Returns the config of the object.</p>
<p>An object config is a Python dictionary (serializable)
containing the information needed to re-instantiate it.</p></div>
</dd>
</dl>
</dd>
<dt id="likelihood.models.deep.autoencoders.GetInsights"><code class="flex name class">
<span>class <span class="ident">GetInsights</span></span>
<span>(</span><span>model: <a title="likelihood.models.deep.autoencoders.AutoClassifier" href="#likelihood.models.deep.autoencoders.AutoClassifier">AutoClassifier</a>,<br>inputs: numpy.ndarray)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GetInsights:
    def __init__(self, model: AutoClassifier, inputs: np.ndarray) -&gt; None:
        self.inputs = inputs
        self.model = model
        self.encoder_layer = self.model.encoder.layers[0]
        self.decoder_layer = self.model.decoder.layers[0]
        self.classifier_layer = self.model.classifier.layers[-2]
        self.encoder_weights = self.encoder_layer.get_weights()[0]
        self.decoder_weights = self.decoder_layer.get_weights()[0]
        self.classifier_weights = self.classifier_layer.get_weights()[0]
        colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)

        by_hsv = sorted(
            (tuple(mcolors.rgb_to_hsv(mcolors.to_rgba(color)[:3])), name)
            for name, color in colors.items()
        )
        self.sorted_names = [name for hsv, name in by_hsv if hsv[1] &gt; 0.4 and hsv[2] &gt;= 0.4]
        random.shuffle(self.sorted_names)

    def predictor_analyzer(
        self,
        frac=None,
        cmap: str = &#34;viridis&#34;,
        aspect: str = &#34;auto&#34;,
        highlight: bool = True,
        **kwargs,
    ) -&gt; None:
        self._viz_weights(cmap=cmap, aspect=aspect, highlight=highlight, **kwargs)
        inputs = self.inputs.copy()
        y_labels = kwargs.get(&#34;y_labels&#34;, None)
        if frac:
            n = int(frac * self.inputs.shape[0])
            indexes = np.random.choice(np.arange(inputs.shape[0]), n, replace=False)
            inputs = inputs[indexes]
        inputs[np.isnan(inputs)] = 0.0
        encoded = self.model.encoder(inputs)
        reconstructed = self.model.decoder(encoded)
        combined = tf.concat([reconstructed, encoded], axis=1)
        self.classification = self.model.classifier(combined).numpy().argmax(axis=1)
        ax = plt.subplot(1, 2, 1)
        plt.imshow(self.inputs, cmap=cmap, aspect=aspect)
        plt.colorbar()
        plt.title(&#34;Original Data&#34;)
        plt.subplot(1, 2, 2, sharex=ax, sharey=ax)
        plt.imshow(reconstructed, cmap=cmap, aspect=aspect)
        plt.colorbar()
        plt.title(&#34;Decoder Layer Reconstruction&#34;)
        plt.show()

        self._get_tsne_repr(inputs=inputs, frac=frac)
        self._viz_tsne_repr(c=self.classification)

        self.data = pd.DataFrame(encoded, columns=[f&#34;Feature {i}&#34; for i in range(encoded.shape[1])])
        self.data_input = pd.DataFrame(
            inputs,
            columns=(
                [f&#34;Feature {i}&#34; for i in range(inputs.shape[1])] if y_labels is None else y_labels
            ),
        )
        self.data[&#34;class&#34;] = self.classification
        self.data_input[&#34;class&#34;] = self.classification
        radviz(self.data, &#34;class&#34;, color=self.colors)
        plt.title(&#34;Radviz Visualization of Latent Space&#34;)
        plt.show()

        radviz(self.data_input, &#34;class&#34;, color=self.colors)
        plt.title(&#34;Radviz Visualization of Input Data&#34;)
        plt.show()
        return self._statistics(self.data_input)

    def _statistics(self, data_input: DataFrame, **kwargs) -&gt; DataFrame:
        data = data_input.copy(deep=True)

        if not pd.api.types.is_string_dtype(data[&#34;class&#34;]):
            data[&#34;class&#34;] = data[&#34;class&#34;].astype(str)

        data.ffill(inplace=True)
        grouped_data = data.groupby(&#34;class&#34;)

        numerical_stats = grouped_data.agg([&#34;mean&#34;, &#34;min&#34;, &#34;max&#34;, &#34;std&#34;, &#34;median&#34;])
        numerical_stats.columns = [&#34;_&#34;.join(col).strip() for col in numerical_stats.columns.values]

        def get_mode(x):
            mode_series = x.mode()
            return mode_series.iloc[0] if not mode_series.empty else None

        mode_stats = grouped_data.apply(get_mode, include_groups=False)
        mode_stats.columns = [f&#34;{col}_mode&#34; for col in mode_stats.columns]
        combined_stats = pd.concat([numerical_stats, mode_stats], axis=1)

        return combined_stats.T

    def _viz_weights(
        self, cmap: str = &#34;viridis&#34;, aspect: str = &#34;auto&#34;, highlight: bool = True, **kwargs
    ) -&gt; None:
        title = kwargs.get(&#34;title&#34;, &#34;Encoder Layer Weights (Dense Layer)&#34;)
        y_labels = kwargs.get(&#34;y_labels&#34;, None)
        cmap_highlight = kwargs.get(&#34;cmap_highlight&#34;, &#34;Pastel1&#34;)
        highlight_mask = np.zeros_like(self.encoder_weights, dtype=bool)

        plt.imshow(self.encoder_weights, cmap=cmap, aspect=aspect)
        plt.colorbar()
        plt.title(title)
        if y_labels is not None:
            plt.yticks(ticks=np.arange(self.encoder_weights.shape[0]), labels=y_labels)
        if highlight:
            for i, j in enumerate(self.encoder_weights.argmax(axis=1)):
                highlight_mask[i, j] = True
            plt.imshow(
                np.ma.masked_where(~highlight_mask, self.encoder_weights),
                cmap=cmap_highlight,
                alpha=0.5,
                aspect=aspect,
            )
        plt.show()

    def _get_tsne_repr(self, inputs=None, frac=None) -&gt; None:
        if inputs is None:
            inputs = self.inputs.copy()
            if frac:
                n = int(frac * self.inputs.shape[0])
                indexes = np.random.choice(np.arange(inputs.shape[0]), n, replace=False)
                inputs = inputs[indexes]
            inputs[np.isnan(inputs)] = 0.0
        self.latent_representations = inputs @ self.encoder_weights

        tsne = TSNE(n_components=2)
        self.reduced_data_tsne = tsne.fit_transform(self.latent_representations)

    def _viz_tsne_repr(self, **kwargs) -&gt; None:
        c = kwargs.get(&#34;c&#34;, None)
        self.colors = (
            kwargs.get(&#34;colors&#34;, self.sorted_names[: len(np.unique(c))]) if c is not None else None
        )
        plt.scatter(
            self.reduced_data_tsne[:, 0],
            self.reduced_data_tsne[:, 1],
            cmap=matplotlib.colors.ListedColormap(self.colors) if c is not None else None,
            c=c,
        )
        if c is not None:
            cb = plt.colorbar()
            loc = np.arange(0, max(c), max(c) / float(len(self.colors)))
            cb.set_ticks(loc)
            cb.set_ticklabels(np.unique(c))
        plt.title(&#34;t-SNE Visualization of Latent Space&#34;)
        plt.xlabel(&#34;t-SNE 1&#34;)
        plt.ylabel(&#34;t-SNE 2&#34;)
        plt.show()</code></pre>
</details>
<div class="desc"></div>
<h3>Methods</h3>
<dl>
<dt id="likelihood.models.deep.autoencoders.GetInsights.predictor_analyzer"><code class="name flex">
<span>def <span class="ident">predictor_analyzer</span></span>(<span>self,<br>frac=None,<br>cmap: str = 'viridis',<br>aspect: str = 'auto',<br>highlight: bool = True,<br>**kwargs) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predictor_analyzer(
    self,
    frac=None,
    cmap: str = &#34;viridis&#34;,
    aspect: str = &#34;auto&#34;,
    highlight: bool = True,
    **kwargs,
) -&gt; None:
    self._viz_weights(cmap=cmap, aspect=aspect, highlight=highlight, **kwargs)
    inputs = self.inputs.copy()
    y_labels = kwargs.get(&#34;y_labels&#34;, None)
    if frac:
        n = int(frac * self.inputs.shape[0])
        indexes = np.random.choice(np.arange(inputs.shape[0]), n, replace=False)
        inputs = inputs[indexes]
    inputs[np.isnan(inputs)] = 0.0
    encoded = self.model.encoder(inputs)
    reconstructed = self.model.decoder(encoded)
    combined = tf.concat([reconstructed, encoded], axis=1)
    self.classification = self.model.classifier(combined).numpy().argmax(axis=1)
    ax = plt.subplot(1, 2, 1)
    plt.imshow(self.inputs, cmap=cmap, aspect=aspect)
    plt.colorbar()
    plt.title(&#34;Original Data&#34;)
    plt.subplot(1, 2, 2, sharex=ax, sharey=ax)
    plt.imshow(reconstructed, cmap=cmap, aspect=aspect)
    plt.colorbar()
    plt.title(&#34;Decoder Layer Reconstruction&#34;)
    plt.show()

    self._get_tsne_repr(inputs=inputs, frac=frac)
    self._viz_tsne_repr(c=self.classification)

    self.data = pd.DataFrame(encoded, columns=[f&#34;Feature {i}&#34; for i in range(encoded.shape[1])])
    self.data_input = pd.DataFrame(
        inputs,
        columns=(
            [f&#34;Feature {i}&#34; for i in range(inputs.shape[1])] if y_labels is None else y_labels
        ),
    )
    self.data[&#34;class&#34;] = self.classification
    self.data_input[&#34;class&#34;] = self.classification
    radviz(self.data, &#34;class&#34;, color=self.colors)
    plt.title(&#34;Radviz Visualization of Latent Space&#34;)
    plt.show()

    radviz(self.data_input, &#34;class&#34;, color=self.colors)
    plt.title(&#34;Radviz Visualization of Input Data&#34;)
    plt.show()
    return self._statistics(self.data_input)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="likelihood.models.deep" href="index.html">likelihood.models.deep</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="likelihood.models.deep.autoencoders.build_model" href="#likelihood.models.deep.autoencoders.build_model">build_model</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.call_existing_code" href="#likelihood.models.deep.autoencoders.call_existing_code">call_existing_code</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.setup_model" href="#likelihood.models.deep.autoencoders.setup_model">setup_model</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.suppress_warnings" href="#likelihood.models.deep.autoencoders.suppress_warnings">suppress_warnings</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="likelihood.models.deep.autoencoders.AutoClassifier" href="#likelihood.models.deep.autoencoders.AutoClassifier">AutoClassifier</a></code></h4>
<ul class="">
<li><code><a title="likelihood.models.deep.autoencoders.AutoClassifier.build" href="#likelihood.models.deep.autoencoders.AutoClassifier.build">build</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.AutoClassifier.call" href="#likelihood.models.deep.autoencoders.AutoClassifier.call">call</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.AutoClassifier.from_config" href="#likelihood.models.deep.autoencoders.AutoClassifier.from_config">from_config</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.AutoClassifier.get_config" href="#likelihood.models.deep.autoencoders.AutoClassifier.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.models.deep.autoencoders.GetInsights" href="#likelihood.models.deep.autoencoders.GetInsights">GetInsights</a></code></h4>
<ul class="">
<li><code><a title="likelihood.models.deep.autoencoders.GetInsights.predictor_analyzer" href="#likelihood.models.deep.autoencoders.GetInsights.predictor_analyzer">predictor_analyzer</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.5</a>.</p>
</footer>
</body>
</html>
