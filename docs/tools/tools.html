<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>likelihood.tools.tools API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>likelihood.tools.tools</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import math
import os
import pickle
from typing import Callable, Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import yaml
from numpy import ndarray
from pandas.core.frame import DataFrame

# -------------------------------------------------------------------------

&#34;&#34;&#34;
Data Science from Scratch, Second Edition, by Joel Grus (O&#39;Reilly).Copyright 2019 Joel Grus, 978-1-492-04113-9
&#34;&#34;&#34;


def minibatches(dataset: List, batch_size: int, shuffle: bool = True) -&gt; List:
    &#34;&#34;&#34;Generates &#39;batch_size&#39;-sized minibatches from the dataset

    Parameters
    ----------
    dataset : `List`
    batch_size : `int`
    shuffle : `bool`

    &#34;&#34;&#34;

    # start indexes 0, batch_size, 2 * batch_size, ...
    batch_starts = [start for start in range(0, len(dataset), batch_size)]

    if shuffle:
        np.random.shuffle(batch_starts)  # shuffle the batches

    for start in batch_starts:
        end = start + batch_size
        yield dataset[start:end]


def difference_quotient(f: Callable, x: float, h: float) -&gt; Callable:
    &#34;&#34;&#34;Calculates the difference quotient of &#39;f&#39; evaluated at x and x + h

    Parameters
    ----------
    f(x) : `Callable` function
    x : `float`
    h : `float`

    Returns
    -------
    `(f(x + h) - f(x)) / h`

    &#34;&#34;&#34;

    return (f(x + h) - f(x)) / h


def partial_difference_quotient(f: Callable, v: ndarray, i: int, h: float):
    &#34;&#34;&#34;Calculates the partial difference quotient of `f`

    Parameters
    ----------
    `f(x0,...,xi-th)` : `Callable` function
    v : `Vector` or `np.array`
    h : `float`

    Returns
    -------
    the `i-th` partial difference quotient of `f` at `v`

    &#34;&#34;&#34;

    w = [
        v_j + (h if j == i else 0) for j, v_j in enumerate(v)  # add h to just the ith element of v
    ]
    return (f(w) - f(v)) / h


def estimate_gradient(f: Callable, v: ndarray, h: float = 1e-4):
    &#34;&#34;&#34;Calculates the gradient of `f` at `v`

    Parameters
    ----------
    `f(x0,...,xi-th)` : `Callable` function
    v : `Vector` or `np.array`
    h : `float`. By default it is set to `1e-4`

    &#34;&#34;&#34;
    return [partial_difference_quotient(f, v, i, h) for i in range(len(v))]


# -------------------------------------------------------------------------


def generate_feature_yaml(
    df: DataFrame, ignore_features: List[str] = None, yaml_string: bool = False
) -&gt; Dict | str:
    &#34;&#34;&#34;
    Generate a YAML string containing information about ordinal, numeric, and categorical features
    based on the given DataFrame.

    Args:
        df (`pd.DataFrame`): The DataFrame containing the data.
        ignore_features (List[`str`]): A list of features to ignore.
        yaml_string (`bool`): If `True`, return the result as a YAML formatted string. Otherwise, return it as a dictionary. Default is `False`.

    Returns:
        `Dict` | `str`: A dictionary with four keys (&#39;ordinal_features&#39;, &#39;numeric_features&#39;, &#39;categorical_features&#39;, &#39;ignore_features&#39;)
        mapping to lists of feature names. Or a YAML formatted string if `yaml_string` is `True`.
    &#34;&#34;&#34;
    feature_info = {
        &#34;ordinal_features&#34;: [],
        &#34;numeric_features&#34;: [],
        &#34;categorical_features&#34;: [],
        &#34;ignore_features&#34;: [],
    }

    for col in df.columns:
        if ignore_features and col in ignore_features:
            continue

        if pd.api.types.is_numeric_dtype(df[col]):
            feature_info[&#34;numeric_features&#34;].append(col)
        elif pd.api.types.is_object_dtype(df[col]) or pd.api.types.is_categorical_dtype(df[col]):
            feature_info[&#34;categorical_features&#34;].append(col)
        elif pd.api.types.is_integer_dtype(df[col]):
            feature_info[&#34;ordinal_features&#34;].append(col)
        elif pd.api.types.is_float_dtype(df[col]):
            feature_info[&#34;ordinal_features&#34;].append(col)
        elif pd.api.types.is_bool_dtype(df[col]):
            feature_info[&#34;ordinal_features&#34;].append(col)
        else:
            print(f&#34;Unknown type for feature {col}&#34;)
        feature_info[&#34;ignore_features&#34;] = ignore_features

    if yaml_string:
        return yaml.dump(feature_info, default_flow_style=False)
    else:
        return feature_info


# a function that calculates the percentage of missing values per column is defined
def cal_missing_values(df: DataFrame) -&gt; None:
    col = df.columns
    print(&#34;Total size : &#34;, &#34;{:,}&#34;.format(len(df)))
    for i in col:
        print(
            str(i) + &#34; : &#34; f&#34;{(df.isnull().sum()[i]/(df.isnull().sum()[i]+df[i].count()))*100:.2f}%&#34;
        )


def calculate_probability(x: ndarray, points: int = 1, cond: bool = True) -&gt; ndarray:
    &#34;&#34;&#34;Calculates the probability of the data

    Parameters
    ----------
    x : `np.array`
        An array containing the data.
    points : `int`
        An integer value. By default it is set to `1`.
    cond : `bool`
        A boolean value. By default it is set to `True`.

    Returns
    -------
    p : `np.array`
        An array containing the probability of the data.

    &#34;&#34;&#34;

    p = []

    f = cdf(x)[0]
    for i in range(len(x)):
        p.append(f(x[i]))
    p = np.array(p)
    if cond:
        if np.prod(p[-points]) &gt; 1:
            print(&#34;\nThe probability of the data cannot be calculated.\n&#34;)
        else:
            if np.prod(p[-points]) &lt; 0:
                print(&#34;\nThe probability of the data cannot be calculated.\n&#34;)
            else:
                print(
                    &#34;The model has a probability of {:.2f}% of being correct&#34;.format(
                        np.prod(p[-points]) * 100
                    )
                )
    else:
        if np.sum(p[-points]) &lt; 0:
            print(&#34;\nThe probability of the data cannot be calculated.\n&#34;)
        else:
            if np.sum(p[-points]) &gt; 1:
                print(&#34;\nThe probability of the data cannot be calculated.\n&#34;)
            else:
                print(
                    &#34;The model has a probability of {:.2f}% of being correct&#34;.format(
                        np.sum(p[-points]) * 100
                    )
                )
    return p


def cdf(
    x: ndarray, poly: int = 9, inv: bool = False, plot: bool = False, savename: str = None
) -&gt; ndarray:
    &#34;&#34;&#34;Calculates the cumulative distribution function of the data

    Parameters
    ----------
    x : `np.array`
        An array containing the data.
    poly : `int`
        An integer value. By default it is set to `9`.
    inv : `bool`
        A boolean value. By default it is set to `False`.

    Returns
    -------
    cdf_ : `np.array`
        An array containing the cumulative distribution function.

    &#34;&#34;&#34;

    cdf_ = np.cumsum(x) / np.sum(x)

    ox = np.sort(x)
    I = np.ones(len(ox))
    M = np.triu(I)
    df = np.dot(ox, M)
    df_ = df / np.max(df)

    if inv:
        fit = np.polyfit(df_, ox, poly)
        f = np.poly1d(fit)
    else:
        fit = np.polyfit(ox, df_, poly)
        f = np.poly1d(fit)

    if plot:
        if inv:
            plt.plot(df_, ox, &#34;o&#34;, label=&#34;inv cdf&#34;)
            plt.plot(df_, f(df_), &#34;r--&#34;, label=&#34;fit&#34;)
            plt.title(&#34;Quantile Function&#34;)
            plt.xlabel(&#34;Probability&#34;)
            plt.ylabel(&#34;Value&#34;)
            plt.legend()
            if savename != None:
                plt.savefig(savename, dpi=300)
            plt.show()
        else:
            plt.plot(ox, cdf_, &#34;o&#34;, label=&#34;cdf&#34;)
            plt.plot(ox, f(ox), &#34;r--&#34;, label=&#34;fit&#34;)
            plt.title(&#34;Cumulative Distribution Function&#34;)
            plt.xlabel(&#34;Value&#34;)
            plt.ylabel(&#34;Probability&#34;)
            plt.legend()
            if savename != None:
                plt.savefig(savename, dpi=300)
            plt.show()

    return f, cdf_, ox


class corr:
    &#34;&#34;&#34;Calculates the correlation of the data

    Parameters
    ----------
    x : `np.array`
        An array containing the data.
    y : `np.array`
        An array containing the data.

    Returns
    -------
    z : `np.array`
        An array containing the correlation of `x` and `y`.

    &#34;&#34;&#34;

    __slots__ = [&#34;x&#34;, &#34;y&#34;, &#34;result&#34;, &#34;z&#34;]

    def __init__(self, x: ndarray, y: ndarray):
        self.x = x
        self.y = y
        self.result = np.correlate(x, y, mode=&#34;full&#34;)
        self.z = self.result[self.result.size // 2 :]
        self.z = self.z / float(np.abs(self.z).max())

    def plot(self):
        plt.plot(range(len(self.z)), self.z, label=&#34;Correlation&#34;)
        plt.legend()
        plt.show()

    def __call__(self):
        return self.z


class autocorr:
    &#34;&#34;&#34;Calculates the autocorrelation of the data

    Parameters
    ----------
    x : `np.array`
        An array containing the data.

    Returns
    -------
    z : `np.array`
        An array containing the autocorrelation of the data.

    &#34;&#34;&#34;

    __slots__ = [&#34;x&#34;, &#34;result&#34;, &#34;z&#34;]

    def __init__(self, x: ndarray):
        self.x = x
        self.result = np.correlate(x, x, mode=&#34;full&#34;)
        self.z = self.result[self.result.size // 2 :]
        self.z = self.z / float(np.abs(self.z).max())

    def plot(self):
        plt.plot(range(len(self.z)), self.z, label=&#34;Autocorrelation&#34;)
        plt.legend()
        plt.show()

    def __call__(self):
        return self.z


def fft_denoise(dataset: ndarray, sigma: float = 0, mode: bool = True) -&gt; Tuple[ndarray, float]:
    &#34;&#34;&#34;Performs the noise removal using the Fast Fourier Transform

    Parameters
    ----------
    dataset : `np.array`
        An array containing the noised data.
    sigma : `float`
        A `float` between `0` and `1`. By default it is set to `0`.
    mode : `bool`
        A boolean value. By default it is set to `True`.

    Returns
    -------
    dataset : `np.array`
        An array containing the denoised data.
    period : `float`
        period of the function described by the dataset

    &#34;&#34;&#34;
    dataset_ = dataset.copy()
    for i in range(dataset.shape[0]):
        n = dataset.shape[1]
        fhat = np.fft.fft(dataset[i, :], n)
        freq = (1 / n) * np.arange(n)
        L = np.arange(1, np.floor(n / 2), dtype=&#34;int&#34;)
        PSD = fhat * np.conj(fhat) / n
        indices = PSD &gt; np.mean(PSD) + sigma * np.std(PSD)
        PSDclean = PSD * indices  # Zero out all others
        fhat = indices * fhat
        ffilt = np.fft.ifft(fhat)  # Inverse FFT for filtered time signal
        dataset_[i, :] = ffilt.real
        # Calculate the period of the signal
        period = 1 / (2 * freq[L][np.argmax(fhat[L])])
        if mode:
            print(f&#34;The {i+1}-th row of the dataset has been denoised.&#34;)
            print(f&#34;The period is {round(period, 4)}&#34;)
    return dataset_, period


def get_period(dataset: ndarray) -&gt; float:
    &#34;&#34;&#34;Calculates the periodicity of a `dataset`

    Args:
        dataset (`ndarray`): the `dataset` describing the function over which the period is calculated

    Returns:
        `float`: period of the function described by the `dataset`
    &#34;&#34;&#34;
    n = dataset.size
    fhat = np.fft.fft(dataset, n)
    freq = (1 / n) * np.arange(n)
    L = np.arange(1, np.floor(n / 2), dtype=&#34;int&#34;)
    PSD = fhat * np.conj(fhat) / n
    indices = PSD &gt; np.mean(PSD) + np.std(PSD)
    fhat = indices * fhat
    period = 1 / (2 * freq[L][np.argmax(fhat[L])])
    return period


def sigmoide_inv(y: float) -&gt; float:
    &#34;&#34;&#34;Calculates the inverse of the sigmoid function

    Args:
        y (`float`): the number to evaluate the function

    Returns:
        `float`: value of evaluated function
    &#34;&#34;&#34;

    return math.log(y / (1 - y))


def sigmoide(x: float) -&gt; float:

    return 1 / (1 + math.exp(-x))


class LogisticRegression:
    &#34;&#34;&#34;class implementing multiple logistic regression&#34;&#34;&#34;

    __slots__ = [&#34;importance&#34;, &#34;X&#34;, &#34;y&#34;, &#34;w&#34;]

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;The class initializer&#34;&#34;&#34;

        self.importance = []

    def fit(self, dataset: ndarray, values: ndarray) -&gt; None:
        &#34;&#34;&#34;Performs linear multiple model training

        Parameters
        ----------
        dataset : `np.array`
            An array containing the scaled data.
        values : `np.ndarray`
            A set of values returned by the linear function.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.

        &#34;&#34;&#34;

        self.X = dataset
        self.y = values

        U, S, VT = np.linalg.svd(self.X, full_matrices=False)

        inverse_sig = np.vectorize(sigmoide_inv)
        self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ inverse_sig(self.y)

        if self.y.shape[1] &gt; 1:
            for row in self.w:
                self.importance.append(np.around(np.max(row), decimals=8))
        else:
            for i in range(self.X.shape[0]):
                a = np.around(self.w[i], decimals=8)
                self.importance.append(a)

    def predict(self, datapoints: ndarray) -&gt; ndarray:
        &#34;&#34;&#34;
        Performs predictions for a set of points

        Parameters
        ----------
        datapoints : `np.array`
            An array containing the values of the independent variable.

        &#34;&#34;&#34;
        sig = np.vectorize(sigmoide)

        return sig(np.array(self.importance) @ datapoints)

    def get_importances(self, print_important_features: bool = False) -&gt; ndarray:
        &#34;&#34;&#34;
        Returns the important features

        Parameters
        ----------
        print_important_features : `bool`
            determines whether or not are printed on the screen. By default it is set to `False`.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.


        &#34;&#34;&#34;
        if print_important_features:
            for i, a in enumerate(self.importance):
                print(f&#34;The importance of the {i+1} feature is {a}&#34;)
        return np.array(self.importance)


class LinearRegression:
    &#34;&#34;&#34;class implementing multiple linear regression&#34;&#34;&#34;

    __slots__ = [&#34;importance&#34;, &#34;X&#34;, &#34;y&#34;, &#34;w&#34;]

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;The class initializer&#34;&#34;&#34;

        self.importance = []

    def fit(self, dataset: ndarray, values: ndarray, verbose: bool = False) -&gt; None:
        &#34;&#34;&#34;Performs linear multiple model training

        Parameters
        ----------
        dataset : `np.array`
            An array containing the scaled data.
        values : `np.ndarray`
            A set of values returned by the linear function.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.

        &#34;&#34;&#34;

        self.X = dataset
        self.y = values

        U, S, VT = np.linalg.svd(self.X, full_matrices=False)
        self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ self.y

        for i in range(self.X.shape[0]):
            a = np.around(self.w[i], decimals=8)
            self.importance.append(a)

        if verbose:
            print(&#34;\nSummary:&#34;)
            print(&#34;--------&#34;)
            print(&#34;\nParameters:&#34;, np.array(self.importance).shape)
            print(&#34;RMSE: {:.4f}&#34;.format(mean_square_error(self.y, self.predict(self.X))))

    def predict(self, datapoints: ndarray) -&gt; ndarray:
        &#34;&#34;&#34;
        Performs predictions for a set of points

        Parameters
        ----------
        datapoints : `np.array`
            An array containing the values of the independent variable.

        &#34;&#34;&#34;
        return np.array(self.importance) @ datapoints

    def get_importances(self, print_important_features: bool = False) -&gt; ndarray:
        &#34;&#34;&#34;
        Returns the important features

        Parameters
        ----------
        print_important_features : `bool`
            determines whether or not are printed on the screen. By default it is set to `False`.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.


        &#34;&#34;&#34;
        if print_important_features:
            for i, a in enumerate(self.importance):
                print(f&#34;The importance of the {i+1} feature is {a}&#34;)
        return np.array(self.importance)


def cal_average(y: ndarray, alpha: float = 1):
    &#34;&#34;&#34;Calculates the moving average of the data

    Parameters
    ----------
    y : `np.array`
        An array containing the data.
    alpha : `float`
        A `float` between `0` and `1`. By default it is set to `1`.

    Returns
    -------
    average : `float`
        The average of the data.

    &#34;&#34;&#34;

    n = int(alpha * len(y))
    w = np.ones(n) / n
    average = np.convolve(y, w, mode=&#34;same&#34;) / np.convolve(np.ones_like(y), w, mode=&#34;same&#34;)
    return average


class DataScaler:
    &#34;&#34;&#34;numpy array `scaler` and `rescaler`&#34;&#34;&#34;

    __slots__ = [&#34;dataset_&#34;, &#34;_n&#34;, &#34;data_scaled&#34;, &#34;values&#34;, &#34;transpose&#34;]

    def __init__(self, dataset: ndarray, n: int = 1) -&gt; None:
        &#34;&#34;&#34;Initializes the parameters required for scaling the data&#34;&#34;&#34;
        self.dataset_ = dataset.copy()
        self._n = n

    def rescale(self) -&gt; ndarray:
        &#34;&#34;&#34;Perform a standard rescaling of the data

        Returns
        -------
        data_scaled : `np.array`
            An array containing the scaled data.
        &#34;&#34;&#34;

        mu = []
        sigma = []
        fitting = []
        self.data_scaled = np.copy(self.dataset_)
        try:
            xaxis = range(self.dataset_.shape[1])
        except:
            error_type = &#34;IndexError&#34;
            msg = &#34;Trying to access an item at an invalid index.&#34;
            print(f&#34;{error_type}: {msg}&#34;)
            return None
        if self.dataset_.shape[0] &gt; self.dataset_.shape[1]:
            self.dataset_ = self.dataset_.T
            self.transpose = True
        else:
            self.transpose = False
        for i in range(self.dataset_.shape[0]):
            if self._n != None:
                fit = np.polyfit(xaxis, self.dataset_[i, :], self._n)
                f = np.poly1d(fit)
                poly = f(xaxis)
                fitting.append(f)
                self.data_scaled[i, :] += -poly
            else:
                fitting.append(0.0)
            mu.append(np.min(self.data_scaled[i, :]))
            if np.max(self.data_scaled[i, :]) != 0:
                sigma.append(np.max(self.data_scaled[i, :]) - mu[i])
            else:
                sigma.append(1)

            self.data_scaled[i, :] = 2 * ((self.data_scaled[i, :] - mu[i]) / sigma[i]) - 1

        self.values = [mu, sigma, fitting]

        return self.data_scaled

    def scale(self, dataset_: ndarray) -&gt; ndarray:
        &#34;&#34;&#34;Performs the inverse operation to the rescale function

        Parameters
        ----------
        dataset_ : `np.array`
            An array containing the scaled values.

        Returns
        -------
        dataset_ : `np.array`
            An array containing the rescaled data.
        &#34;&#34;&#34;
        if self.transpose:
            dataset_ = dataset_.T
        for i in range(dataset_.shape[0]):
            dataset_[i, :] += 1
            dataset_[i, :] /= 2
            dataset_[i, :] = dataset_[i, :] * self.values[1][i]
            dataset_[i, :] += self.values[0][i]
            dataset_[i, :] += self.values[2][i](range(dataset_.shape[1]))

        return dataset_


def generate_series(n: int, n_steps: int, incline: bool = True):
    &#34;&#34;&#34;Function that generates `n` series of length `n_steps`&#34;&#34;&#34;
    freq1, freq2, offsets1, offsets2 = np.random.rand(4, n, 1)

    if incline:
        slope = np.random.rand(n, 1)
    else:
        slope = 0.0
        offsets2 = 1

    time = np.linspace(0, 1, n_steps)
    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  # wave 1
    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))  # + wave 2
    series += 0.7 * (np.random.rand(n, n_steps) - 0.5)  # + noise
    series += 5 * slope * time + 2 * (offsets2 - offsets1) * time ** (1 - offsets2)
    series = series
    return series.astype(np.float32)


def mean_square_error(y_true: ndarray, y_pred: ndarray, print_error: bool = False):
    &#34;&#34;&#34;Calculates the Root Mean Squared Error

    Parameters
    ----------
    y_true : `np.array`
        An array containing the true values.
    y_pred : `np.array`
        An array containing the predicted values.

    Returns
    -------
    RMSE : `float`
        The Root Mean Squared Error.

    &#34;&#34;&#34;
    if print_error:
        print(f&#34;The RMSE is {np.sqrt(np.mean((y_true - y_pred)**2))}&#34;)

    return np.sqrt(np.mean((y_true - y_pred) ** 2))


class DataFrameEncoder:
    &#34;&#34;&#34;Allows encoding and decoding Dataframes&#34;&#34;&#34;

    __slots__ = [&#34;_df&#34;, &#34;_names&#34;, &#34;_encode_columns&#34;, &#34;encoding_list&#34;, &#34;decoding_list&#34;]

    def __init__(self, data: DataFrame) -&gt; None:
        &#34;&#34;&#34;Sets the columns of the `DataFrame`&#34;&#34;&#34;
        self._df = data.copy()
        self._names = data.columns
        self._encode_columns = []
        self.encoding_list = []
        self.decoding_list = []

    def load_config(self, path_to_dictionaries: str = &#34;./&#34;, **kwargs) -&gt; None:
        &#34;&#34;&#34;Loads dictionaries from a given directory

        Keyword Arguments:
        ----------
        - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
        &#34;&#34;&#34;
        dictionary_name = (
            kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
        )
        with open(os.path.join(path_to_dictionaries, dictionary_name + &#34;.pkl&#34;), &#34;rb&#34;) as file:
            labelencoder = pickle.load(file)
        self.encoding_list = labelencoder[0]
        self.decoding_list = labelencoder[1]
        self._encode_columns = labelencoder[2]
        print(&#34;Configuration successfully uploaded&#34;)

    def train(self, path_to_save: str, **kwargs) -&gt; None:
        &#34;&#34;&#34;Trains the encoders and decoders using the `DataFrame`&#34;&#34;&#34;
        save_mode = kwargs[&#34;save_mode&#34;] if &#34;save_mode&#34; in kwargs else True
        dictionary_name = (
            kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
        )
        for i in self._names:
            if self._df[i].dtype == &#34;object&#34;:
                self._encode_columns.append(i)
                column_index = range(len(self._df[i].unique()))
                column_keys = self._df[i].unique()
                encode_dict = dict(zip(column_keys, column_index))
                decode_dict = dict(zip(column_index, column_keys))
                self._df[i] = self._df[i].apply(
                    self._code_transformation_to, dictionary_list=encode_dict
                )
                self.encoding_list.append(encode_dict)
                self.decoding_list.append(decode_dict)
        if save_mode:
            self._save_encoder(path_to_save, dictionary_name)

    def encode(self, path_to_save: str = &#34;./&#34;, **kwargs) -&gt; DataFrame:
        &#34;&#34;&#34;Encodes the `object` type columns of the dataframe

        Keyword Arguments:
        ----------
        - save_mode (`bool`): An optional integer parameter. By default it is set to `True`
        - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
        &#34;&#34;&#34;
        if len(self.encoding_list) == 0:
            self.train(path_to_save, **kwargs)
            return self._df

        else:
            print(&#34;Configuration detected&#34;)
            for num, colname in enumerate(self._encode_columns):
                if self._df[colname].dtype == &#34;object&#34;:
                    encode_dict = self.encoding_list[num]
                    self._df[colname] = self._df[colname].apply(
                        self._code_transformation_to, dictionary_list=encode_dict
                    )
            return self._df

    def decode(self) -&gt; DataFrame:
        &#34;&#34;&#34;Decodes the `int` type columns of the `DataFrame`&#34;&#34;&#34;
        j = 0
        df_decoded = self._df.copy()
        try:
            number_of_columns = len(self.decoding_list[j])
            for i in self._encode_columns:
                if df_decoded[i].dtype == &#34;int64&#34;:
                    df_decoded[i] = df_decoded[i].apply(
                        self._code_transformation_to, dictionary_list=self.decoding_list[j]
                    )
                    j += 1
            return df_decoded
        except AttributeError as e:
            warning_type = &#34;UserWarning&#34;
            msg = &#34;It is not possible to decode the dataframe, since it has not been encoded&#34;
            msg += &#34;Error: {%s}&#34; % e
            print(f&#34;{warning_type}: {msg}&#34;)

    def get_dictionaries(self) -&gt; Tuple[List[dict], List[dict]]:
        &#34;&#34;&#34;Allows to return the `list` of dictionaries for `encoding` and `decoding`&#34;&#34;&#34;
        try:
            return self.encoding_list, self.decoding_list
        except ValueError as e:
            warning_type = &#34;UserWarning&#34;
            msg = &#34;It is not possible to return the list of dictionaries as they have not been created.&#34;
            msg += &#34;Error: {%s}&#34; % e
            print(f&#34;{warning_type}: {msg}&#34;)

    def _save_encoder(self, path_to_save: str, dictionary_name: str) -&gt; None:
        &#34;&#34;&#34;Method to serialize the `encoding_list`, `decoding_list` and `_encode_columns` list&#34;&#34;&#34;
        with open(path_to_save + dictionary_name + &#34;.pkl&#34;, &#34;wb&#34;) as f:
            pickle.dump([self.encoding_list, self.decoding_list, self._encode_columns], f)

    def _code_transformation_to(self, character: str, dictionary_list: List[dict]) -&gt; int:
        &#34;&#34;&#34;Auxiliary function to perform data transformation using a dictionary

        Parameters
        ----------
        character : `str`
            A character data type.
        dictionary_list : List[`dict`]
            An object of dictionary type.

        Returns
        -------
        dict_type[`character`] or `np.nan` if dict_type[`character`] doesn&#39;t exist.
        &#34;&#34;&#34;
        try:
            return dictionary_list[character]
        except:
            return np.nan


class PerformanceMeasures:
    &#34;&#34;&#34;Class with methods to measure performance&#34;&#34;&#34;

    def __init__(self) -&gt; None:
        pass

    # Performance measure Res_T
    def f_mean(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; None:
        n = len(labels)

        F_vec = self._f1_score(y_true, y_pred, labels=labels)
        a = np.sum(F_vec)

        for i in range(len(F_vec)):
            print(&#34;F-measure of label &#34;, labels[i], &#34; -&gt; &#34;, F_vec[i])

        print(&#34;Mean of F-measure -&gt; &#34;, a / n)

    # Performance measure Res_P
    def resp(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; None:
        # We initialize sum counters
        sum1 = 0
        sum2 = 0

        # Calculamos T_C
        T_C = len(y_true)
        for i in range(len(labels)):
            # We calculate instances of the classes and their F-measures
            sum1 += (1 - ((y_true == labels[i]).sum() / T_C)) * self._fi_measure(
                y_true, y_pred, labels, i
            )
            sum2 += 1 - ((y_true == labels[i]).sum()) / T_C

        # Print the metric corresponding to the prediction vector
        print(&#34;Metric Res_p -&gt;&#34;, sum1 / sum2)

    def _fi_measure(self, y_true: ndarray, y_pred: ndarray, labels: list, i: int) -&gt; int:
        F_vec = self._f1_score(y_true, y_pred, labels=labels)

        return F_vec[i]  # We return the position of the f1-score corresponding to the label

    # Summary of the labels predicted
    def _summary_pred(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; None:
        count_mat = self._confu_mat(y_true, y_pred, labels)
        print(&#34;        &#34;, end=&#34;&#34;)
        for i in range(len(labels)):
            print(&#34;|--&#34;, labels[i], &#34;--&#34;, end=&#34;&#34;)
            if i + 1 == len(labels):
                print(&#34;|&#34;, end=&#34;&#34;)
        for i in range(len(labels)):
            print(&#34;&#34;)
            print(&#34;|--&#34;, labels[i], &#34;--|&#34;, end=&#34;&#34;)
            for j in range(len(labels)):
                if j != 0:
                    print(&#34; &#34;, end=&#34;&#34;)
                print(&#34;  &#34;, int(count_mat[i, j]), &#34;  &#34;, end=&#34;&#34;)

    def _f1_score(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; ndarray:
        f1_vec = np.zeros(len(labels))

        # Calculate confusion mat
        count_mat = self._confu_mat(y_true, y_pred, labels)

        # sums over columns
        sum1 = np.sum(count_mat, axis=0)
        # sums over rows
        sum2 = np.sum(count_mat, axis=1)
        # Iterate over labels to calculate f1 scores of each one
        for i in range(len(labels)):
            precision = count_mat[i, i] / (sum1[i])
            recall = count_mat[i, i] / (sum2[i])

            f1_vec[i] = 2 * ((precision * recall) / (precision + recall))

        return f1_vec

    # Returns confusion matrix of predictions
    def _confu_mat(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; ndarray:
        labels = np.array(labels)
        count_mat = np.zeros((len(labels), len(labels)))

        for i in range(len(labels)):
            for j in range(len(y_pred)):
                if y_pred[j] == labels[i]:
                    if y_pred[j] == y_true[j]:
                        count_mat[i, i] += 1
                    else:
                        x = np.where(labels == y_true[j])
                        count_mat[i, x[0]] += 1

        return count_mat


class OneHotEncoder:
    &#34;&#34;&#34;
    Class used to encode categorical variables.
    It receives an array of integers and returns a binary array using the one-hot encoding method.
    &#34;&#34;&#34;

    __slots__ = [&#34;x&#34;]

    def __init__(self) -&gt; None:
        pass

    def encode(self, x: ndarray | list):
        self.x = x

        if not isinstance(self.x, ndarray):
            self.x = np.array(self.x)  # If not numpy array then convert it

        y = np.zeros(
            (self.x.size, self.x.max() + 1)
        )  # Build matrix of (size num of entries) x (max value + 1)

        y[np.arange(self.x.size), self.x] = 1  # Label with ones

        return y

    def decode(self, x: ndarray | list) -&gt; ndarray:
        if not isinstance(x, ndarray):
            x = np.array(x)  # If not numpy array then convert it

        # Regresamos los valores max de cada renglon
        y = np.argmax(x, axis=1)

        return y


class FeatureSelection:
    &#34;&#34;&#34;
    Generate the data graph using a variation of the feature selection algorithm..

    - The method `get_digraph` returns the network based on the feature selection method.
    &#34;&#34;&#34;

    __slots__ = [&#34;not_features&#34;, &#34;X&#34;, &#34;all_features_imp_graph&#34;, &#34;w_dict&#34;, &#34;scaler&#34;]

    def __init__(self, not_features: list[str] = []) -&gt; None:
        &#34;&#34;&#34;The initializer of the class. The initial parameter is a list of strings with variables to discard.&#34;&#34;&#34;
        self.not_features: List[str] = not_features
        self.all_features_imp_graph: List[Tuple] = []
        self.w_dict = dict()

    def get_digraph(self, dataset: DataFrame, n_importances: int) -&gt; str:
        &#34;&#34;&#34;
        Get directed graph showing importance of features.

        Args:
            dataset (`DataFrame`): Dataset to be used for generating the graph.
            n_importances (`int`): Number of top importances to show in the graph.

        Returns:
            A string representation of the directed graph.
        &#34;&#34;&#34;
        # Assign and clean dataset
        self._load_data(dataset)

        curr_dataset = self.X
        columns = list(curr_dataset.columns)

        # We construct string from causal_graph
        feature_string = &#34; digraph { &#34;
        for column in columns:
            feature_string += column + &#34;; &#34;

        numeric_df = curr_dataset.select_dtypes(include=&#34;number&#34;)
        self.scaler = DataScaler(numeric_df.copy().to_numpy().T, n=None)
        numeric_scaled = self.scaler.rescale()
        numeric_df = pd.DataFrame(numeric_scaled.T, columns=numeric_df.columns)
        curr_dataset[numeric_df.columns] = numeric_df

        # We construct dictionary to save index for scaling
        numeric_dict = dict(zip(list(numeric_df.columns), range(len(list(numeric_df.columns)))))

        # Iterate over all the columns to obtain their importances.
        for index_column, column in enumerate(columns):

            # Variable to predict
            Y = curr_dataset[column]

            # We check whether it is numerical or categorical.
            column_type = Y.dtype
            if column_type != &#34;object&#34;:
                # Linear regression model
                Model = LinearRegression()

                # Auxiliary dataset without the column in question
                X_aux = curr_dataset.drop([column], axis=1)

                # We encode
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)
                # We train

                Model.fit(encoded_df.to_numpy().T, Y.to_numpy().T)
                # We obtain importance
                importance = Model.get_importances()
                w = Model.w
            else:
                Model = LogisticRegression()
                num_unique_entries = curr_dataset[column].nunique()

                quick_encoder = DataFrameEncoder(Y.to_frame())
                encoded_Y = quick_encoder.encode(save_mode=False)

                # Mapping to one-hot
                one_hot = OneHotEncoder()
                train_y = one_hot.encode(encoded_Y[column])
                # PASSING 0 -&gt; 0.5 and 1 -&gt; 0.73105
                for i in range(len(train_y)):
                    for j in range(num_unique_entries):
                        if train_y[i][j] == 1.0:
                            train_y[i][j] = 0.73105
                        else:
                            train_y[i][j] = 0.5

                # Delete the column in question
                X_aux = curr_dataset.drop([column], axis=1)

                # We encode
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)

                # We train
                Model.fit(encoded_df.to_numpy().T, train_y)

                # We obtain importance
                importance = Model.get_importances()
                w = Model.w

            # We obtain the $n$ most important ones
            top_n_indexes = sorted(
                range(len(importance)), key=lambda i: importance[i], reverse=True
            )[:n_importances]

            # We build the string for the column in question
            names_cols = list(X_aux.columns)
            # We store the indices, values and column names in a list of tuples.
            features_imp_node = [
                (names_cols[top_n_indexes[i]], importance[top_n_indexes[i]])
                for i in range(n_importances)
            ]
            # We store w&#39;s for predictions

            if column_type != &#34;object&#34;:
                self.w_dict[column] = (w, None, names_cols, dfe, numeric_dict)
            else:
                self.w_dict[column] = (w, quick_encoder, names_cols, dfe, numeric_dict)
            # Add to general list
            self.all_features_imp_graph.append((column, features_imp_node))
            # We format it
            for i in top_n_indexes:
                feature_string += names_cols[i] + &#34; -&gt; &#34;

            feature_string += column + &#34;; &#34;

        return feature_string + &#34;} &#34;

    def _load_data(self, dataset: DataFrame):
        # Assign data and clean dataset of unneeded columns

        if len(self.not_features) &gt; 0:
            # We remove unnecessary columns
            self.X = dataset.drop(columns=self.not_features)

        else:
            self.X = dataset

        self.X.replace([np.inf, -np.inf], np.nan, inplace=True)
        self.X.replace(&#34; &#34;, np.nan, inplace=True)
        self.X.dropna(inplace=True)
        self.X = self.X.reset_index()
        self.X = self.X.drop(columns=[&#34;index&#34;])


def check_nan_inf(df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;Check for `NaN` and `Inf` values in the `DataFrame`. If any are found, raise an error.&#34;&#34;&#34;
    nan_values = df.isnull().values.any()
    count = np.isinf(df.select_dtypes(include=&#34;number&#34;)).values.sum()
    print(&#34;There are null values : &#34;, nan_values)
    print(&#34;It contains &#34; + str(count) + &#34; infinite values&#34;)
    if nan_values:
        warning_type = &#34;UserWarning&#34;
        msg = &#34;Some rows may have been deleted due to the existence of nan values.&#34;
        print(f&#34;{warning_type}: {msg}&#34;)
        print(&#34;Missing values correctly removed : &#34;, &#34;{:,}&#34;.format(df.isnull().values.sum()))
        df = df.dropna()

    return df


# -------------------------------------------------------------------------
if __name__ == &#34;__main__&#34;:
    y_true = np.array([1, 2, 2, 1, 1])
    y_pred = np.array([1, 1, 2, 2, 1])

    labels = [1, 2]
    helper = PerformanceMeasures()
    helper._summary_pred(y_true, y_pred, labels)
    print(helper._f1_score(y_true, y_pred, labels))

    # Use DataFrameEncoder
    # Create a DataFrame
    data = {&#34;Name&#34;: [&#34;John&#34;, &#34;Alice&#34;, &#34;Bob&#34;], &#34;Age&#34;: [25, 30, 35]}
    import pandas as pd

    df = pd.DataFrame(data)
    # Instantiate DataFrameEncoder
    dfe = DataFrameEncoder(df)
    # Encode the dataframe
    encoded_df = dfe.encode()
    # Decode the dataframe
    decoded_df = dfe.decode()

    # Instantiate DataFrameEncoder
    # Use load_config method
    dfe2 = DataFrameEncoder(df)
    dfe2.load_config()

    encoded_df2 = dfe2.encode()
    # Decode the dataframe
    decoded_df2 = dfe2.decode()
    # Check if the loaded dictionaries match the original ones
    assert dfe.encoding_list == dfe2.encoding_list
    assert dfe.decoding_list == dfe2.decoding_list

    # Generate data
    x = np.random.rand(3, 100)
    y = 0.1 * x[0, :] + 0.4 * x[1, :] + 0.5 * x[2, :] + 0.1
    linear_model = LinearRegression()
    linear_model.fit(x, y)
    importance = linear_model.get_importances()
    y_hat = linear_model.predict(x)

    # Graph the data for visualization
    plt.plot(x[0, :], y, &#34;o&#34;, label=&#34;Original Data&#34;)
    plt.plot(x[0, :], y_hat, &#34;x&#34;, label=&#34;$\hat{y}$&#34;)
    plt.legend()
    plt.xlabel(&#34;$x$&#34;)
    plt.ylabel(&#34;$y, $\hat{y}$&#34;)
    plt.show()

    a = generate_series(1, 40, incline=False)
    # Graph the data for visualization
    plt.plot(range(len(a[0, :])), a[0, :], label=&#34;Original Data&#34;)
    plt.legend()
    plt.xlabel(&#34;Time periods&#34;)
    plt.ylabel(&#34;$y(t)$&#34;)
    plt.show()

    a_denoise, _ = fft_denoise(a)

    plt.plot(range(len(a_denoise[0, :])), a_denoise[0, :], label=&#34;Denoise Data&#34;)
    plt.legend()
    plt.xlabel(&#34;Time periods&#34;)
    plt.ylabel(&#34;$y(t)$&#34;)
    plt.show()

    # Calculate the autocorrelation of the data
    z = autocorr(a[0, :])
    z.plot()
    # print(z())

    N = 1000
    mu = np.random.uniform(0, 10.0)
    sigma = np.random.uniform(0.1, 1.0)
    x = np.random.normal(mu, sigma, N)
    f, cdf_, ox = cdf(x, plot=True)
    invf, cdf_, ox = cdf(x, plot=True, inv=True)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="likelihood.tools.tools.cal_average"><code class="name flex">
<span>def <span class="ident">cal_average</span></span>(<span>y: numpy.ndarray, alpha: float = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the moving average of the data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the data.</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>A <code>float</code> between <code>0</code> and <code>1</code>. By default it is set to <code>1</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>average</code></strong> :&ensp;<code>float</code></dt>
<dd>The average of the data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cal_average(y: ndarray, alpha: float = 1):
    &#34;&#34;&#34;Calculates the moving average of the data

    Parameters
    ----------
    y : `np.array`
        An array containing the data.
    alpha : `float`
        A `float` between `0` and `1`. By default it is set to `1`.

    Returns
    -------
    average : `float`
        The average of the data.

    &#34;&#34;&#34;

    n = int(alpha * len(y))
    w = np.ones(n) / n
    average = np.convolve(y, w, mode=&#34;same&#34;) / np.convolve(np.ones_like(y), w, mode=&#34;same&#34;)
    return average</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.cal_missing_values"><code class="name flex">
<span>def <span class="ident">cal_missing_values</span></span>(<span>df: pandas.core.frame.DataFrame) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cal_missing_values(df: DataFrame) -&gt; None:
    col = df.columns
    print(&#34;Total size : &#34;, &#34;{:,}&#34;.format(len(df)))
    for i in col:
        print(
            str(i) + &#34; : &#34; f&#34;{(df.isnull().sum()[i]/(df.isnull().sum()[i]+df[i].count()))*100:.2f}%&#34;
        )</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.calculate_probability"><code class="name flex">
<span>def <span class="ident">calculate_probability</span></span>(<span>x: numpy.ndarray, points: int = 1, cond: bool = True) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the probability of the data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the data.</dd>
<dt><strong><code>points</code></strong> :&ensp;<code>int</code></dt>
<dd>An integer value. By default it is set to <code>1</code>.</dd>
<dt><strong><code>cond</code></strong> :&ensp;<code>bool</code></dt>
<dd>A boolean value. By default it is set to <code>True</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the probability of the data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_probability(x: ndarray, points: int = 1, cond: bool = True) -&gt; ndarray:
    &#34;&#34;&#34;Calculates the probability of the data

    Parameters
    ----------
    x : `np.array`
        An array containing the data.
    points : `int`
        An integer value. By default it is set to `1`.
    cond : `bool`
        A boolean value. By default it is set to `True`.

    Returns
    -------
    p : `np.array`
        An array containing the probability of the data.

    &#34;&#34;&#34;

    p = []

    f = cdf(x)[0]
    for i in range(len(x)):
        p.append(f(x[i]))
    p = np.array(p)
    if cond:
        if np.prod(p[-points]) &gt; 1:
            print(&#34;\nThe probability of the data cannot be calculated.\n&#34;)
        else:
            if np.prod(p[-points]) &lt; 0:
                print(&#34;\nThe probability of the data cannot be calculated.\n&#34;)
            else:
                print(
                    &#34;The model has a probability of {:.2f}% of being correct&#34;.format(
                        np.prod(p[-points]) * 100
                    )
                )
    else:
        if np.sum(p[-points]) &lt; 0:
            print(&#34;\nThe probability of the data cannot be calculated.\n&#34;)
        else:
            if np.sum(p[-points]) &gt; 1:
                print(&#34;\nThe probability of the data cannot be calculated.\n&#34;)
            else:
                print(
                    &#34;The model has a probability of {:.2f}% of being correct&#34;.format(
                        np.sum(p[-points]) * 100
                    )
                )
    return p</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.cdf"><code class="name flex">
<span>def <span class="ident">cdf</span></span>(<span>x: numpy.ndarray, poly: int = 9, inv: bool = False, plot: bool = False, savename: str = None) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the cumulative distribution function of the data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the data.</dd>
<dt><strong><code>poly</code></strong> :&ensp;<code>int</code></dt>
<dd>An integer value. By default it is set to <code>9</code>.</dd>
<dt><strong><code>inv</code></strong> :&ensp;<code>bool</code></dt>
<dd>A boolean value. By default it is set to <code>False</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>cdf_</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the cumulative distribution function.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cdf(
    x: ndarray, poly: int = 9, inv: bool = False, plot: bool = False, savename: str = None
) -&gt; ndarray:
    &#34;&#34;&#34;Calculates the cumulative distribution function of the data

    Parameters
    ----------
    x : `np.array`
        An array containing the data.
    poly : `int`
        An integer value. By default it is set to `9`.
    inv : `bool`
        A boolean value. By default it is set to `False`.

    Returns
    -------
    cdf_ : `np.array`
        An array containing the cumulative distribution function.

    &#34;&#34;&#34;

    cdf_ = np.cumsum(x) / np.sum(x)

    ox = np.sort(x)
    I = np.ones(len(ox))
    M = np.triu(I)
    df = np.dot(ox, M)
    df_ = df / np.max(df)

    if inv:
        fit = np.polyfit(df_, ox, poly)
        f = np.poly1d(fit)
    else:
        fit = np.polyfit(ox, df_, poly)
        f = np.poly1d(fit)

    if plot:
        if inv:
            plt.plot(df_, ox, &#34;o&#34;, label=&#34;inv cdf&#34;)
            plt.plot(df_, f(df_), &#34;r--&#34;, label=&#34;fit&#34;)
            plt.title(&#34;Quantile Function&#34;)
            plt.xlabel(&#34;Probability&#34;)
            plt.ylabel(&#34;Value&#34;)
            plt.legend()
            if savename != None:
                plt.savefig(savename, dpi=300)
            plt.show()
        else:
            plt.plot(ox, cdf_, &#34;o&#34;, label=&#34;cdf&#34;)
            plt.plot(ox, f(ox), &#34;r--&#34;, label=&#34;fit&#34;)
            plt.title(&#34;Cumulative Distribution Function&#34;)
            plt.xlabel(&#34;Value&#34;)
            plt.ylabel(&#34;Probability&#34;)
            plt.legend()
            if savename != None:
                plt.savefig(savename, dpi=300)
            plt.show()

    return f, cdf_, ox</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.check_nan_inf"><code class="name flex">
<span>def <span class="ident">check_nan_inf</span></span>(<span>df: pandas.core.frame.DataFrame) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Check for <code>NaN</code> and <code>Inf</code> values in the <code>DataFrame</code>. If any are found, raise an error.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_nan_inf(df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;Check for `NaN` and `Inf` values in the `DataFrame`. If any are found, raise an error.&#34;&#34;&#34;
    nan_values = df.isnull().values.any()
    count = np.isinf(df.select_dtypes(include=&#34;number&#34;)).values.sum()
    print(&#34;There are null values : &#34;, nan_values)
    print(&#34;It contains &#34; + str(count) + &#34; infinite values&#34;)
    if nan_values:
        warning_type = &#34;UserWarning&#34;
        msg = &#34;Some rows may have been deleted due to the existence of nan values.&#34;
        print(f&#34;{warning_type}: {msg}&#34;)
        print(&#34;Missing values correctly removed : &#34;, &#34;{:,}&#34;.format(df.isnull().values.sum()))
        df = df.dropna()

    return df</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.difference_quotient"><code class="name flex">
<span>def <span class="ident">difference_quotient</span></span>(<span>f: Callable, x: float, h: float) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the difference quotient of 'f' evaluated at x and x + h</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>f(x) : <code>Callable</code> function</dt>
<dt><strong><code>x</code></strong> :&ensp;<code>float</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>h</code></strong> :&ensp;<code>float</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>(f(x + h) - f(x)) / h</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def difference_quotient(f: Callable, x: float, h: float) -&gt; Callable:
    &#34;&#34;&#34;Calculates the difference quotient of &#39;f&#39; evaluated at x and x + h

    Parameters
    ----------
    f(x) : `Callable` function
    x : `float`
    h : `float`

    Returns
    -------
    `(f(x + h) - f(x)) / h`

    &#34;&#34;&#34;

    return (f(x + h) - f(x)) / h</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.estimate_gradient"><code class="name flex">
<span>def <span class="ident">estimate_gradient</span></span>(<span>f: Callable, v: numpy.ndarray, h: float = 0.0001)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the gradient of <code>f</code> at <code>v</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><code>f(x0,...,xi-th)</code> : <code>Callable</code> function</dt>
<dt><strong><code>v</code></strong> :&ensp;<code>Vector</code> or <code>np.array</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>h</code></strong> :&ensp;<code>float`. By default it is set to `1e-4</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def estimate_gradient(f: Callable, v: ndarray, h: float = 1e-4):
    &#34;&#34;&#34;Calculates the gradient of `f` at `v`

    Parameters
    ----------
    `f(x0,...,xi-th)` : `Callable` function
    v : `Vector` or `np.array`
    h : `float`. By default it is set to `1e-4`

    &#34;&#34;&#34;
    return [partial_difference_quotient(f, v, i, h) for i in range(len(v))]</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.fft_denoise"><code class="name flex">
<span>def <span class="ident">fft_denoise</span></span>(<span>dataset: numpy.ndarray, sigma: float = 0, mode: bool = True) ‑> Tuple[numpy.ndarray, float]</span>
</code></dt>
<dd>
<div class="desc"><p>Performs the noise removal using the Fast Fourier Transform</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the noised data.</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>float</code></dt>
<dd>A <code>float</code> between <code>0</code> and <code>1</code>. By default it is set to <code>0</code>.</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>A boolean value. By default it is set to <code>True</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the denoised data.</dd>
<dt><strong><code>period</code></strong> :&ensp;<code>float</code></dt>
<dd>period of the function described by the dataset</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fft_denoise(dataset: ndarray, sigma: float = 0, mode: bool = True) -&gt; Tuple[ndarray, float]:
    &#34;&#34;&#34;Performs the noise removal using the Fast Fourier Transform

    Parameters
    ----------
    dataset : `np.array`
        An array containing the noised data.
    sigma : `float`
        A `float` between `0` and `1`. By default it is set to `0`.
    mode : `bool`
        A boolean value. By default it is set to `True`.

    Returns
    -------
    dataset : `np.array`
        An array containing the denoised data.
    period : `float`
        period of the function described by the dataset

    &#34;&#34;&#34;
    dataset_ = dataset.copy()
    for i in range(dataset.shape[0]):
        n = dataset.shape[1]
        fhat = np.fft.fft(dataset[i, :], n)
        freq = (1 / n) * np.arange(n)
        L = np.arange(1, np.floor(n / 2), dtype=&#34;int&#34;)
        PSD = fhat * np.conj(fhat) / n
        indices = PSD &gt; np.mean(PSD) + sigma * np.std(PSD)
        PSDclean = PSD * indices  # Zero out all others
        fhat = indices * fhat
        ffilt = np.fft.ifft(fhat)  # Inverse FFT for filtered time signal
        dataset_[i, :] = ffilt.real
        # Calculate the period of the signal
        period = 1 / (2 * freq[L][np.argmax(fhat[L])])
        if mode:
            print(f&#34;The {i+1}-th row of the dataset has been denoised.&#34;)
            print(f&#34;The period is {round(period, 4)}&#34;)
    return dataset_, period</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.generate_feature_yaml"><code class="name flex">
<span>def <span class="ident">generate_feature_yaml</span></span>(<span>df: pandas.core.frame.DataFrame, ignore_features: List[str] = None, yaml_string: bool = False) ‑> Union[Dict, str]</span>
</code></dt>
<dd>
<div class="desc"><p>Generate a YAML string containing information about ordinal, numeric, and categorical features
based on the given DataFrame.</p>
<h2 id="args">Args</h2>
<p>df (<code>pd.DataFrame</code>): The DataFrame containing the data.
ignore_features (List[<code>str</code>]): A list of features to ignore.
yaml_string (<code>bool</code>): If <code>True</code>, return the result as a YAML formatted string. Otherwise, return it as a dictionary. Default is <code>False</code>.</p>
<h2 id="returns">Returns</h2>
<p><code>Dict</code> | <code>str</code>: A dictionary with four keys ('ordinal_features', 'numeric_features', 'categorical_features', 'ignore_features')
mapping to lists of feature names. Or a YAML formatted string if <code>yaml_string</code> is <code>True</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_feature_yaml(
    df: DataFrame, ignore_features: List[str] = None, yaml_string: bool = False
) -&gt; Dict | str:
    &#34;&#34;&#34;
    Generate a YAML string containing information about ordinal, numeric, and categorical features
    based on the given DataFrame.

    Args:
        df (`pd.DataFrame`): The DataFrame containing the data.
        ignore_features (List[`str`]): A list of features to ignore.
        yaml_string (`bool`): If `True`, return the result as a YAML formatted string. Otherwise, return it as a dictionary. Default is `False`.

    Returns:
        `Dict` | `str`: A dictionary with four keys (&#39;ordinal_features&#39;, &#39;numeric_features&#39;, &#39;categorical_features&#39;, &#39;ignore_features&#39;)
        mapping to lists of feature names. Or a YAML formatted string if `yaml_string` is `True`.
    &#34;&#34;&#34;
    feature_info = {
        &#34;ordinal_features&#34;: [],
        &#34;numeric_features&#34;: [],
        &#34;categorical_features&#34;: [],
        &#34;ignore_features&#34;: [],
    }

    for col in df.columns:
        if ignore_features and col in ignore_features:
            continue

        if pd.api.types.is_numeric_dtype(df[col]):
            feature_info[&#34;numeric_features&#34;].append(col)
        elif pd.api.types.is_object_dtype(df[col]) or pd.api.types.is_categorical_dtype(df[col]):
            feature_info[&#34;categorical_features&#34;].append(col)
        elif pd.api.types.is_integer_dtype(df[col]):
            feature_info[&#34;ordinal_features&#34;].append(col)
        elif pd.api.types.is_float_dtype(df[col]):
            feature_info[&#34;ordinal_features&#34;].append(col)
        elif pd.api.types.is_bool_dtype(df[col]):
            feature_info[&#34;ordinal_features&#34;].append(col)
        else:
            print(f&#34;Unknown type for feature {col}&#34;)
        feature_info[&#34;ignore_features&#34;] = ignore_features

    if yaml_string:
        return yaml.dump(feature_info, default_flow_style=False)
    else:
        return feature_info</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.generate_series"><code class="name flex">
<span>def <span class="ident">generate_series</span></span>(<span>n: int, n_steps: int, incline: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Function that generates <code>n</code> series of length <code>n_steps</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_series(n: int, n_steps: int, incline: bool = True):
    &#34;&#34;&#34;Function that generates `n` series of length `n_steps`&#34;&#34;&#34;
    freq1, freq2, offsets1, offsets2 = np.random.rand(4, n, 1)

    if incline:
        slope = np.random.rand(n, 1)
    else:
        slope = 0.0
        offsets2 = 1

    time = np.linspace(0, 1, n_steps)
    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  # wave 1
    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))  # + wave 2
    series += 0.7 * (np.random.rand(n, n_steps) - 0.5)  # + noise
    series += 5 * slope * time + 2 * (offsets2 - offsets1) * time ** (1 - offsets2)
    series = series
    return series.astype(np.float32)</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.get_period"><code class="name flex">
<span>def <span class="ident">get_period</span></span>(<span>dataset: numpy.ndarray) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the periodicity of a <code>dataset</code></p>
<h2 id="args">Args</h2>
<p>dataset (<code>ndarray</code>): the <code>dataset</code> describing the function over which the period is calculated</p>
<h2 id="returns">Returns</h2>
<p><code>float</code>: period of the function described by the <code>dataset</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_period(dataset: ndarray) -&gt; float:
    &#34;&#34;&#34;Calculates the periodicity of a `dataset`

    Args:
        dataset (`ndarray`): the `dataset` describing the function over which the period is calculated

    Returns:
        `float`: period of the function described by the `dataset`
    &#34;&#34;&#34;
    n = dataset.size
    fhat = np.fft.fft(dataset, n)
    freq = (1 / n) * np.arange(n)
    L = np.arange(1, np.floor(n / 2), dtype=&#34;int&#34;)
    PSD = fhat * np.conj(fhat) / n
    indices = PSD &gt; np.mean(PSD) + np.std(PSD)
    fhat = indices * fhat
    period = 1 / (2 * freq[L][np.argmax(fhat[L])])
    return period</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.mean_square_error"><code class="name flex">
<span>def <span class="ident">mean_square_error</span></span>(<span>y_true: numpy.ndarray, y_pred: numpy.ndarray, print_error: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the Root Mean Squared Error</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>y_true</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the true values.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the predicted values.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>RMSE</code></strong> :&ensp;<code>float</code></dt>
<dd>The Root Mean Squared Error.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean_square_error(y_true: ndarray, y_pred: ndarray, print_error: bool = False):
    &#34;&#34;&#34;Calculates the Root Mean Squared Error

    Parameters
    ----------
    y_true : `np.array`
        An array containing the true values.
    y_pred : `np.array`
        An array containing the predicted values.

    Returns
    -------
    RMSE : `float`
        The Root Mean Squared Error.

    &#34;&#34;&#34;
    if print_error:
        print(f&#34;The RMSE is {np.sqrt(np.mean((y_true - y_pred)**2))}&#34;)

    return np.sqrt(np.mean((y_true - y_pred) ** 2))</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.minibatches"><code class="name flex">
<span>def <span class="ident">minibatches</span></span>(<span>dataset: List, batch_size: int, shuffle: bool = True) ‑> List</span>
</code></dt>
<dd>
<div class="desc"><p>Generates 'batch_size'-sized minibatches from the dataset</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>List</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>shuffle</code></strong> :&ensp;<code>bool</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def minibatches(dataset: List, batch_size: int, shuffle: bool = True) -&gt; List:
    &#34;&#34;&#34;Generates &#39;batch_size&#39;-sized minibatches from the dataset

    Parameters
    ----------
    dataset : `List`
    batch_size : `int`
    shuffle : `bool`

    &#34;&#34;&#34;

    # start indexes 0, batch_size, 2 * batch_size, ...
    batch_starts = [start for start in range(0, len(dataset), batch_size)]

    if shuffle:
        np.random.shuffle(batch_starts)  # shuffle the batches

    for start in batch_starts:
        end = start + batch_size
        yield dataset[start:end]</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.partial_difference_quotient"><code class="name flex">
<span>def <span class="ident">partial_difference_quotient</span></span>(<span>f: Callable, v: numpy.ndarray, i: int, h: float)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the partial difference quotient of <code>f</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><code>f(x0,...,xi-th)</code> : <code>Callable</code> function</dt>
<dt><strong><code>v</code></strong> :&ensp;<code>Vector</code> or <code>np.array</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>h</code></strong> :&ensp;<code>float</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>the <code>i-th</code> partial difference quotient of <code>f</code> at <code>v</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def partial_difference_quotient(f: Callable, v: ndarray, i: int, h: float):
    &#34;&#34;&#34;Calculates the partial difference quotient of `f`

    Parameters
    ----------
    `f(x0,...,xi-th)` : `Callable` function
    v : `Vector` or `np.array`
    h : `float`

    Returns
    -------
    the `i-th` partial difference quotient of `f` at `v`

    &#34;&#34;&#34;

    w = [
        v_j + (h if j == i else 0) for j, v_j in enumerate(v)  # add h to just the ith element of v
    ]
    return (f(w) - f(v)) / h</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.sigmoide"><code class="name flex">
<span>def <span class="ident">sigmoide</span></span>(<span>x: float) ‑> float</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sigmoide(x: float) -&gt; float:

    return 1 / (1 + math.exp(-x))</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.sigmoide_inv"><code class="name flex">
<span>def <span class="ident">sigmoide_inv</span></span>(<span>y: float) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the inverse of the sigmoid function</p>
<h2 id="args">Args</h2>
<p>y (<code>float</code>): the number to evaluate the function</p>
<h2 id="returns">Returns</h2>
<p><code>float</code>: value of evaluated function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sigmoide_inv(y: float) -&gt; float:
    &#34;&#34;&#34;Calculates the inverse of the sigmoid function

    Args:
        y (`float`): the number to evaluate the function

    Returns:
        `float`: value of evaluated function
    &#34;&#34;&#34;

    return math.log(y / (1 - y))</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="likelihood.tools.tools.DataFrameEncoder"><code class="flex name class">
<span>class <span class="ident">DataFrameEncoder</span></span>
<span>(</span><span>data: pandas.core.frame.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Allows encoding and decoding Dataframes</p>
<p>Sets the columns of the <code>DataFrame</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataFrameEncoder:
    &#34;&#34;&#34;Allows encoding and decoding Dataframes&#34;&#34;&#34;

    __slots__ = [&#34;_df&#34;, &#34;_names&#34;, &#34;_encode_columns&#34;, &#34;encoding_list&#34;, &#34;decoding_list&#34;]

    def __init__(self, data: DataFrame) -&gt; None:
        &#34;&#34;&#34;Sets the columns of the `DataFrame`&#34;&#34;&#34;
        self._df = data.copy()
        self._names = data.columns
        self._encode_columns = []
        self.encoding_list = []
        self.decoding_list = []

    def load_config(self, path_to_dictionaries: str = &#34;./&#34;, **kwargs) -&gt; None:
        &#34;&#34;&#34;Loads dictionaries from a given directory

        Keyword Arguments:
        ----------
        - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
        &#34;&#34;&#34;
        dictionary_name = (
            kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
        )
        with open(os.path.join(path_to_dictionaries, dictionary_name + &#34;.pkl&#34;), &#34;rb&#34;) as file:
            labelencoder = pickle.load(file)
        self.encoding_list = labelencoder[0]
        self.decoding_list = labelencoder[1]
        self._encode_columns = labelencoder[2]
        print(&#34;Configuration successfully uploaded&#34;)

    def train(self, path_to_save: str, **kwargs) -&gt; None:
        &#34;&#34;&#34;Trains the encoders and decoders using the `DataFrame`&#34;&#34;&#34;
        save_mode = kwargs[&#34;save_mode&#34;] if &#34;save_mode&#34; in kwargs else True
        dictionary_name = (
            kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
        )
        for i in self._names:
            if self._df[i].dtype == &#34;object&#34;:
                self._encode_columns.append(i)
                column_index = range(len(self._df[i].unique()))
                column_keys = self._df[i].unique()
                encode_dict = dict(zip(column_keys, column_index))
                decode_dict = dict(zip(column_index, column_keys))
                self._df[i] = self._df[i].apply(
                    self._code_transformation_to, dictionary_list=encode_dict
                )
                self.encoding_list.append(encode_dict)
                self.decoding_list.append(decode_dict)
        if save_mode:
            self._save_encoder(path_to_save, dictionary_name)

    def encode(self, path_to_save: str = &#34;./&#34;, **kwargs) -&gt; DataFrame:
        &#34;&#34;&#34;Encodes the `object` type columns of the dataframe

        Keyword Arguments:
        ----------
        - save_mode (`bool`): An optional integer parameter. By default it is set to `True`
        - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
        &#34;&#34;&#34;
        if len(self.encoding_list) == 0:
            self.train(path_to_save, **kwargs)
            return self._df

        else:
            print(&#34;Configuration detected&#34;)
            for num, colname in enumerate(self._encode_columns):
                if self._df[colname].dtype == &#34;object&#34;:
                    encode_dict = self.encoding_list[num]
                    self._df[colname] = self._df[colname].apply(
                        self._code_transformation_to, dictionary_list=encode_dict
                    )
            return self._df

    def decode(self) -&gt; DataFrame:
        &#34;&#34;&#34;Decodes the `int` type columns of the `DataFrame`&#34;&#34;&#34;
        j = 0
        df_decoded = self._df.copy()
        try:
            number_of_columns = len(self.decoding_list[j])
            for i in self._encode_columns:
                if df_decoded[i].dtype == &#34;int64&#34;:
                    df_decoded[i] = df_decoded[i].apply(
                        self._code_transformation_to, dictionary_list=self.decoding_list[j]
                    )
                    j += 1
            return df_decoded
        except AttributeError as e:
            warning_type = &#34;UserWarning&#34;
            msg = &#34;It is not possible to decode the dataframe, since it has not been encoded&#34;
            msg += &#34;Error: {%s}&#34; % e
            print(f&#34;{warning_type}: {msg}&#34;)

    def get_dictionaries(self) -&gt; Tuple[List[dict], List[dict]]:
        &#34;&#34;&#34;Allows to return the `list` of dictionaries for `encoding` and `decoding`&#34;&#34;&#34;
        try:
            return self.encoding_list, self.decoding_list
        except ValueError as e:
            warning_type = &#34;UserWarning&#34;
            msg = &#34;It is not possible to return the list of dictionaries as they have not been created.&#34;
            msg += &#34;Error: {%s}&#34; % e
            print(f&#34;{warning_type}: {msg}&#34;)

    def _save_encoder(self, path_to_save: str, dictionary_name: str) -&gt; None:
        &#34;&#34;&#34;Method to serialize the `encoding_list`, `decoding_list` and `_encode_columns` list&#34;&#34;&#34;
        with open(path_to_save + dictionary_name + &#34;.pkl&#34;, &#34;wb&#34;) as f:
            pickle.dump([self.encoding_list, self.decoding_list, self._encode_columns], f)

    def _code_transformation_to(self, character: str, dictionary_list: List[dict]) -&gt; int:
        &#34;&#34;&#34;Auxiliary function to perform data transformation using a dictionary

        Parameters
        ----------
        character : `str`
            A character data type.
        dictionary_list : List[`dict`]
            An object of dictionary type.

        Returns
        -------
        dict_type[`character`] or `np.nan` if dict_type[`character`] doesn&#39;t exist.
        &#34;&#34;&#34;
        try:
            return dictionary_list[character]
        except:
            return np.nan</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.DataFrameEncoder.decoding_list"><code class="name">var <span class="ident">decoding_list</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.encoding_list"><code class="name">var <span class="ident">encoding_list</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.DataFrameEncoder.decode"><code class="name flex">
<span>def <span class="ident">decode</span></span>(<span>self) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Decodes the <code>int</code> type columns of the <code>DataFrame</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decode(self) -&gt; DataFrame:
    &#34;&#34;&#34;Decodes the `int` type columns of the `DataFrame`&#34;&#34;&#34;
    j = 0
    df_decoded = self._df.copy()
    try:
        number_of_columns = len(self.decoding_list[j])
        for i in self._encode_columns:
            if df_decoded[i].dtype == &#34;int64&#34;:
                df_decoded[i] = df_decoded[i].apply(
                    self._code_transformation_to, dictionary_list=self.decoding_list[j]
                )
                j += 1
        return df_decoded
    except AttributeError as e:
        warning_type = &#34;UserWarning&#34;
        msg = &#34;It is not possible to decode the dataframe, since it has not been encoded&#34;
        msg += &#34;Error: {%s}&#34; % e
        print(f&#34;{warning_type}: {msg}&#34;)</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.encode"><code class="name flex">
<span>def <span class="ident">encode</span></span>(<span>self, path_to_save: str = './', **kwargs) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Encodes the <code>object</code> type columns of the dataframe</p>
<h2 id="keyword-arguments">Keyword Arguments:</h2>
<ul>
<li>save_mode (<code>bool</code>): An optional integer parameter. By default it is set to <code>True</code></li>
<li>dictionary_name (<code>str</code>): An optional string parameter. By default it is set to <code>labelencoder_dictionary</code></li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode(self, path_to_save: str = &#34;./&#34;, **kwargs) -&gt; DataFrame:
    &#34;&#34;&#34;Encodes the `object` type columns of the dataframe

    Keyword Arguments:
    ----------
    - save_mode (`bool`): An optional integer parameter. By default it is set to `True`
    - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
    &#34;&#34;&#34;
    if len(self.encoding_list) == 0:
        self.train(path_to_save, **kwargs)
        return self._df

    else:
        print(&#34;Configuration detected&#34;)
        for num, colname in enumerate(self._encode_columns):
            if self._df[colname].dtype == &#34;object&#34;:
                encode_dict = self.encoding_list[num]
                self._df[colname] = self._df[colname].apply(
                    self._code_transformation_to, dictionary_list=encode_dict
                )
        return self._df</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.get_dictionaries"><code class="name flex">
<span>def <span class="ident">get_dictionaries</span></span>(<span>self) ‑> Tuple[List[dict], List[dict]]</span>
</code></dt>
<dd>
<div class="desc"><p>Allows to return the <code>list</code> of dictionaries for <code>encoding</code> and <code>decoding</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dictionaries(self) -&gt; Tuple[List[dict], List[dict]]:
    &#34;&#34;&#34;Allows to return the `list` of dictionaries for `encoding` and `decoding`&#34;&#34;&#34;
    try:
        return self.encoding_list, self.decoding_list
    except ValueError as e:
        warning_type = &#34;UserWarning&#34;
        msg = &#34;It is not possible to return the list of dictionaries as they have not been created.&#34;
        msg += &#34;Error: {%s}&#34; % e
        print(f&#34;{warning_type}: {msg}&#34;)</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.load_config"><code class="name flex">
<span>def <span class="ident">load_config</span></span>(<span>self, path_to_dictionaries: str = './', **kwargs) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Loads dictionaries from a given directory</p>
<h2 id="keyword-arguments">Keyword Arguments:</h2>
<ul>
<li>dictionary_name (<code>str</code>): An optional string parameter. By default it is set to <code>labelencoder_dictionary</code></li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_config(self, path_to_dictionaries: str = &#34;./&#34;, **kwargs) -&gt; None:
    &#34;&#34;&#34;Loads dictionaries from a given directory

    Keyword Arguments:
    ----------
    - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
    &#34;&#34;&#34;
    dictionary_name = (
        kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
    )
    with open(os.path.join(path_to_dictionaries, dictionary_name + &#34;.pkl&#34;), &#34;rb&#34;) as file:
        labelencoder = pickle.load(file)
    self.encoding_list = labelencoder[0]
    self.decoding_list = labelencoder[1]
    self._encode_columns = labelencoder[2]
    print(&#34;Configuration successfully uploaded&#34;)</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, path_to_save: str, **kwargs) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the encoders and decoders using the <code>DataFrame</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, path_to_save: str, **kwargs) -&gt; None:
    &#34;&#34;&#34;Trains the encoders and decoders using the `DataFrame`&#34;&#34;&#34;
    save_mode = kwargs[&#34;save_mode&#34;] if &#34;save_mode&#34; in kwargs else True
    dictionary_name = (
        kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
    )
    for i in self._names:
        if self._df[i].dtype == &#34;object&#34;:
            self._encode_columns.append(i)
            column_index = range(len(self._df[i].unique()))
            column_keys = self._df[i].unique()
            encode_dict = dict(zip(column_keys, column_index))
            decode_dict = dict(zip(column_index, column_keys))
            self._df[i] = self._df[i].apply(
                self._code_transformation_to, dictionary_list=encode_dict
            )
            self.encoding_list.append(encode_dict)
            self.decoding_list.append(decode_dict)
    if save_mode:
        self._save_encoder(path_to_save, dictionary_name)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.DataScaler"><code class="flex name class">
<span>class <span class="ident">DataScaler</span></span>
<span>(</span><span>dataset: numpy.ndarray, n: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>numpy array <code>scaler</code> and <code>rescaler</code></p>
<p>Initializes the parameters required for scaling the data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataScaler:
    &#34;&#34;&#34;numpy array `scaler` and `rescaler`&#34;&#34;&#34;

    __slots__ = [&#34;dataset_&#34;, &#34;_n&#34;, &#34;data_scaled&#34;, &#34;values&#34;, &#34;transpose&#34;]

    def __init__(self, dataset: ndarray, n: int = 1) -&gt; None:
        &#34;&#34;&#34;Initializes the parameters required for scaling the data&#34;&#34;&#34;
        self.dataset_ = dataset.copy()
        self._n = n

    def rescale(self) -&gt; ndarray:
        &#34;&#34;&#34;Perform a standard rescaling of the data

        Returns
        -------
        data_scaled : `np.array`
            An array containing the scaled data.
        &#34;&#34;&#34;

        mu = []
        sigma = []
        fitting = []
        self.data_scaled = np.copy(self.dataset_)
        try:
            xaxis = range(self.dataset_.shape[1])
        except:
            error_type = &#34;IndexError&#34;
            msg = &#34;Trying to access an item at an invalid index.&#34;
            print(f&#34;{error_type}: {msg}&#34;)
            return None
        if self.dataset_.shape[0] &gt; self.dataset_.shape[1]:
            self.dataset_ = self.dataset_.T
            self.transpose = True
        else:
            self.transpose = False
        for i in range(self.dataset_.shape[0]):
            if self._n != None:
                fit = np.polyfit(xaxis, self.dataset_[i, :], self._n)
                f = np.poly1d(fit)
                poly = f(xaxis)
                fitting.append(f)
                self.data_scaled[i, :] += -poly
            else:
                fitting.append(0.0)
            mu.append(np.min(self.data_scaled[i, :]))
            if np.max(self.data_scaled[i, :]) != 0:
                sigma.append(np.max(self.data_scaled[i, :]) - mu[i])
            else:
                sigma.append(1)

            self.data_scaled[i, :] = 2 * ((self.data_scaled[i, :] - mu[i]) / sigma[i]) - 1

        self.values = [mu, sigma, fitting]

        return self.data_scaled

    def scale(self, dataset_: ndarray) -&gt; ndarray:
        &#34;&#34;&#34;Performs the inverse operation to the rescale function

        Parameters
        ----------
        dataset_ : `np.array`
            An array containing the scaled values.

        Returns
        -------
        dataset_ : `np.array`
            An array containing the rescaled data.
        &#34;&#34;&#34;
        if self.transpose:
            dataset_ = dataset_.T
        for i in range(dataset_.shape[0]):
            dataset_[i, :] += 1
            dataset_[i, :] /= 2
            dataset_[i, :] = dataset_[i, :] * self.values[1][i]
            dataset_[i, :] += self.values[0][i]
            dataset_[i, :] += self.values[2][i](range(dataset_.shape[1]))

        return dataset_</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.DataScaler.data_scaled"><code class="name">var <span class="ident">data_scaled</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.DataScaler.dataset_"><code class="name">var <span class="ident">dataset_</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.DataScaler.transpose"><code class="name">var <span class="ident">transpose</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.DataScaler.values"><code class="name">var <span class="ident">values</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.DataScaler.rescale"><code class="name flex">
<span>def <span class="ident">rescale</span></span>(<span>self) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Perform a standard rescaling of the data</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data_scaled</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the scaled data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rescale(self) -&gt; ndarray:
    &#34;&#34;&#34;Perform a standard rescaling of the data

    Returns
    -------
    data_scaled : `np.array`
        An array containing the scaled data.
    &#34;&#34;&#34;

    mu = []
    sigma = []
    fitting = []
    self.data_scaled = np.copy(self.dataset_)
    try:
        xaxis = range(self.dataset_.shape[1])
    except:
        error_type = &#34;IndexError&#34;
        msg = &#34;Trying to access an item at an invalid index.&#34;
        print(f&#34;{error_type}: {msg}&#34;)
        return None
    if self.dataset_.shape[0] &gt; self.dataset_.shape[1]:
        self.dataset_ = self.dataset_.T
        self.transpose = True
    else:
        self.transpose = False
    for i in range(self.dataset_.shape[0]):
        if self._n != None:
            fit = np.polyfit(xaxis, self.dataset_[i, :], self._n)
            f = np.poly1d(fit)
            poly = f(xaxis)
            fitting.append(f)
            self.data_scaled[i, :] += -poly
        else:
            fitting.append(0.0)
        mu.append(np.min(self.data_scaled[i, :]))
        if np.max(self.data_scaled[i, :]) != 0:
            sigma.append(np.max(self.data_scaled[i, :]) - mu[i])
        else:
            sigma.append(1)

        self.data_scaled[i, :] = 2 * ((self.data_scaled[i, :] - mu[i]) / sigma[i]) - 1

    self.values = [mu, sigma, fitting]

    return self.data_scaled</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.DataScaler.scale"><code class="name flex">
<span>def <span class="ident">scale</span></span>(<span>self, dataset_: numpy.ndarray) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Performs the inverse operation to the rescale function</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset_</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the scaled values.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dataset_</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the rescaled data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scale(self, dataset_: ndarray) -&gt; ndarray:
    &#34;&#34;&#34;Performs the inverse operation to the rescale function

    Parameters
    ----------
    dataset_ : `np.array`
        An array containing the scaled values.

    Returns
    -------
    dataset_ : `np.array`
        An array containing the rescaled data.
    &#34;&#34;&#34;
    if self.transpose:
        dataset_ = dataset_.T
    for i in range(dataset_.shape[0]):
        dataset_[i, :] += 1
        dataset_[i, :] /= 2
        dataset_[i, :] = dataset_[i, :] * self.values[1][i]
        dataset_[i, :] += self.values[0][i]
        dataset_[i, :] += self.values[2][i](range(dataset_.shape[1]))

    return dataset_</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.FeatureSelection"><code class="flex name class">
<span>class <span class="ident">FeatureSelection</span></span>
<span>(</span><span>not_features: list[str] = [])</span>
</code></dt>
<dd>
<div class="desc"><p>Generate the data graph using a variation of the feature selection algorithm..</p>
<ul>
<li>The method <code>get_digraph</code> returns the network based on the feature selection method.</li>
</ul>
<p>The initializer of the class. The initial parameter is a list of strings with variables to discard.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureSelection:
    &#34;&#34;&#34;
    Generate the data graph using a variation of the feature selection algorithm..

    - The method `get_digraph` returns the network based on the feature selection method.
    &#34;&#34;&#34;

    __slots__ = [&#34;not_features&#34;, &#34;X&#34;, &#34;all_features_imp_graph&#34;, &#34;w_dict&#34;, &#34;scaler&#34;]

    def __init__(self, not_features: list[str] = []) -&gt; None:
        &#34;&#34;&#34;The initializer of the class. The initial parameter is a list of strings with variables to discard.&#34;&#34;&#34;
        self.not_features: List[str] = not_features
        self.all_features_imp_graph: List[Tuple] = []
        self.w_dict = dict()

    def get_digraph(self, dataset: DataFrame, n_importances: int) -&gt; str:
        &#34;&#34;&#34;
        Get directed graph showing importance of features.

        Args:
            dataset (`DataFrame`): Dataset to be used for generating the graph.
            n_importances (`int`): Number of top importances to show in the graph.

        Returns:
            A string representation of the directed graph.
        &#34;&#34;&#34;
        # Assign and clean dataset
        self._load_data(dataset)

        curr_dataset = self.X
        columns = list(curr_dataset.columns)

        # We construct string from causal_graph
        feature_string = &#34; digraph { &#34;
        for column in columns:
            feature_string += column + &#34;; &#34;

        numeric_df = curr_dataset.select_dtypes(include=&#34;number&#34;)
        self.scaler = DataScaler(numeric_df.copy().to_numpy().T, n=None)
        numeric_scaled = self.scaler.rescale()
        numeric_df = pd.DataFrame(numeric_scaled.T, columns=numeric_df.columns)
        curr_dataset[numeric_df.columns] = numeric_df

        # We construct dictionary to save index for scaling
        numeric_dict = dict(zip(list(numeric_df.columns), range(len(list(numeric_df.columns)))))

        # Iterate over all the columns to obtain their importances.
        for index_column, column in enumerate(columns):

            # Variable to predict
            Y = curr_dataset[column]

            # We check whether it is numerical or categorical.
            column_type = Y.dtype
            if column_type != &#34;object&#34;:
                # Linear regression model
                Model = LinearRegression()

                # Auxiliary dataset without the column in question
                X_aux = curr_dataset.drop([column], axis=1)

                # We encode
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)
                # We train

                Model.fit(encoded_df.to_numpy().T, Y.to_numpy().T)
                # We obtain importance
                importance = Model.get_importances()
                w = Model.w
            else:
                Model = LogisticRegression()
                num_unique_entries = curr_dataset[column].nunique()

                quick_encoder = DataFrameEncoder(Y.to_frame())
                encoded_Y = quick_encoder.encode(save_mode=False)

                # Mapping to one-hot
                one_hot = OneHotEncoder()
                train_y = one_hot.encode(encoded_Y[column])
                # PASSING 0 -&gt; 0.5 and 1 -&gt; 0.73105
                for i in range(len(train_y)):
                    for j in range(num_unique_entries):
                        if train_y[i][j] == 1.0:
                            train_y[i][j] = 0.73105
                        else:
                            train_y[i][j] = 0.5

                # Delete the column in question
                X_aux = curr_dataset.drop([column], axis=1)

                # We encode
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)

                # We train
                Model.fit(encoded_df.to_numpy().T, train_y)

                # We obtain importance
                importance = Model.get_importances()
                w = Model.w

            # We obtain the $n$ most important ones
            top_n_indexes = sorted(
                range(len(importance)), key=lambda i: importance[i], reverse=True
            )[:n_importances]

            # We build the string for the column in question
            names_cols = list(X_aux.columns)
            # We store the indices, values and column names in a list of tuples.
            features_imp_node = [
                (names_cols[top_n_indexes[i]], importance[top_n_indexes[i]])
                for i in range(n_importances)
            ]
            # We store w&#39;s for predictions

            if column_type != &#34;object&#34;:
                self.w_dict[column] = (w, None, names_cols, dfe, numeric_dict)
            else:
                self.w_dict[column] = (w, quick_encoder, names_cols, dfe, numeric_dict)
            # Add to general list
            self.all_features_imp_graph.append((column, features_imp_node))
            # We format it
            for i in top_n_indexes:
                feature_string += names_cols[i] + &#34; -&gt; &#34;

            feature_string += column + &#34;; &#34;

        return feature_string + &#34;} &#34;

    def _load_data(self, dataset: DataFrame):
        # Assign data and clean dataset of unneeded columns

        if len(self.not_features) &gt; 0:
            # We remove unnecessary columns
            self.X = dataset.drop(columns=self.not_features)

        else:
            self.X = dataset

        self.X.replace([np.inf, -np.inf], np.nan, inplace=True)
        self.X.replace(&#34; &#34;, np.nan, inplace=True)
        self.X.dropna(inplace=True)
        self.X = self.X.reset_index()
        self.X = self.X.drop(columns=[&#34;index&#34;])</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="likelihood.graph.graph.DynamicGraph" href="../graph/graph.html#likelihood.graph.graph.DynamicGraph">DynamicGraph</a></li>
<li><a title="likelihood.models.simulation.SimulationEngine" href="../models/simulation.html#likelihood.models.simulation.SimulationEngine">SimulationEngine</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.FeatureSelection.X"><code class="name">var <span class="ident">X</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.FeatureSelection.all_features_imp_graph"><code class="name">var <span class="ident">all_features_imp_graph</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.FeatureSelection.not_features"><code class="name">var <span class="ident">not_features</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.FeatureSelection.scaler"><code class="name">var <span class="ident">scaler</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.FeatureSelection.w_dict"><code class="name">var <span class="ident">w_dict</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.FeatureSelection.get_digraph"><code class="name flex">
<span>def <span class="ident">get_digraph</span></span>(<span>self, dataset: pandas.core.frame.DataFrame, n_importances: int) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Get directed graph showing importance of features.</p>
<h2 id="args">Args</h2>
<p>dataset (<code>DataFrame</code>): Dataset to be used for generating the graph.
n_importances (<code>int</code>): Number of top importances to show in the graph.</p>
<h2 id="returns">Returns</h2>
<p>A string representation of the directed graph.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_digraph(self, dataset: DataFrame, n_importances: int) -&gt; str:
    &#34;&#34;&#34;
    Get directed graph showing importance of features.

    Args:
        dataset (`DataFrame`): Dataset to be used for generating the graph.
        n_importances (`int`): Number of top importances to show in the graph.

    Returns:
        A string representation of the directed graph.
    &#34;&#34;&#34;
    # Assign and clean dataset
    self._load_data(dataset)

    curr_dataset = self.X
    columns = list(curr_dataset.columns)

    # We construct string from causal_graph
    feature_string = &#34; digraph { &#34;
    for column in columns:
        feature_string += column + &#34;; &#34;

    numeric_df = curr_dataset.select_dtypes(include=&#34;number&#34;)
    self.scaler = DataScaler(numeric_df.copy().to_numpy().T, n=None)
    numeric_scaled = self.scaler.rescale()
    numeric_df = pd.DataFrame(numeric_scaled.T, columns=numeric_df.columns)
    curr_dataset[numeric_df.columns] = numeric_df

    # We construct dictionary to save index for scaling
    numeric_dict = dict(zip(list(numeric_df.columns), range(len(list(numeric_df.columns)))))

    # Iterate over all the columns to obtain their importances.
    for index_column, column in enumerate(columns):

        # Variable to predict
        Y = curr_dataset[column]

        # We check whether it is numerical or categorical.
        column_type = Y.dtype
        if column_type != &#34;object&#34;:
            # Linear regression model
            Model = LinearRegression()

            # Auxiliary dataset without the column in question
            X_aux = curr_dataset.drop([column], axis=1)

            # We encode
            dfe = DataFrameEncoder(X_aux)
            encoded_df = dfe.encode(save_mode=False)
            # We train

            Model.fit(encoded_df.to_numpy().T, Y.to_numpy().T)
            # We obtain importance
            importance = Model.get_importances()
            w = Model.w
        else:
            Model = LogisticRegression()
            num_unique_entries = curr_dataset[column].nunique()

            quick_encoder = DataFrameEncoder(Y.to_frame())
            encoded_Y = quick_encoder.encode(save_mode=False)

            # Mapping to one-hot
            one_hot = OneHotEncoder()
            train_y = one_hot.encode(encoded_Y[column])
            # PASSING 0 -&gt; 0.5 and 1 -&gt; 0.73105
            for i in range(len(train_y)):
                for j in range(num_unique_entries):
                    if train_y[i][j] == 1.0:
                        train_y[i][j] = 0.73105
                    else:
                        train_y[i][j] = 0.5

            # Delete the column in question
            X_aux = curr_dataset.drop([column], axis=1)

            # We encode
            dfe = DataFrameEncoder(X_aux)
            encoded_df = dfe.encode(save_mode=False)

            # We train
            Model.fit(encoded_df.to_numpy().T, train_y)

            # We obtain importance
            importance = Model.get_importances()
            w = Model.w

        # We obtain the $n$ most important ones
        top_n_indexes = sorted(
            range(len(importance)), key=lambda i: importance[i], reverse=True
        )[:n_importances]

        # We build the string for the column in question
        names_cols = list(X_aux.columns)
        # We store the indices, values and column names in a list of tuples.
        features_imp_node = [
            (names_cols[top_n_indexes[i]], importance[top_n_indexes[i]])
            for i in range(n_importances)
        ]
        # We store w&#39;s for predictions

        if column_type != &#34;object&#34;:
            self.w_dict[column] = (w, None, names_cols, dfe, numeric_dict)
        else:
            self.w_dict[column] = (w, quick_encoder, names_cols, dfe, numeric_dict)
        # Add to general list
        self.all_features_imp_graph.append((column, features_imp_node))
        # We format it
        for i in top_n_indexes:
            feature_string += names_cols[i] + &#34; -&gt; &#34;

        feature_string += column + &#34;; &#34;

    return feature_string + &#34;} &#34;</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.LinearRegression"><code class="flex name class">
<span>class <span class="ident">LinearRegression</span></span>
</code></dt>
<dd>
<div class="desc"><p>class implementing multiple linear regression</p>
<p>The class initializer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearRegression:
    &#34;&#34;&#34;class implementing multiple linear regression&#34;&#34;&#34;

    __slots__ = [&#34;importance&#34;, &#34;X&#34;, &#34;y&#34;, &#34;w&#34;]

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;The class initializer&#34;&#34;&#34;

        self.importance = []

    def fit(self, dataset: ndarray, values: ndarray, verbose: bool = False) -&gt; None:
        &#34;&#34;&#34;Performs linear multiple model training

        Parameters
        ----------
        dataset : `np.array`
            An array containing the scaled data.
        values : `np.ndarray`
            A set of values returned by the linear function.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.

        &#34;&#34;&#34;

        self.X = dataset
        self.y = values

        U, S, VT = np.linalg.svd(self.X, full_matrices=False)
        self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ self.y

        for i in range(self.X.shape[0]):
            a = np.around(self.w[i], decimals=8)
            self.importance.append(a)

        if verbose:
            print(&#34;\nSummary:&#34;)
            print(&#34;--------&#34;)
            print(&#34;\nParameters:&#34;, np.array(self.importance).shape)
            print(&#34;RMSE: {:.4f}&#34;.format(mean_square_error(self.y, self.predict(self.X))))

    def predict(self, datapoints: ndarray) -&gt; ndarray:
        &#34;&#34;&#34;
        Performs predictions for a set of points

        Parameters
        ----------
        datapoints : `np.array`
            An array containing the values of the independent variable.

        &#34;&#34;&#34;
        return np.array(self.importance) @ datapoints

    def get_importances(self, print_important_features: bool = False) -&gt; ndarray:
        &#34;&#34;&#34;
        Returns the important features

        Parameters
        ----------
        print_important_features : `bool`
            determines whether or not are printed on the screen. By default it is set to `False`.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.


        &#34;&#34;&#34;
        if print_important_features:
            for i, a in enumerate(self.importance):
                print(f&#34;The importance of the {i+1} feature is {a}&#34;)
        return np.array(self.importance)</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.LinearRegression.X"><code class="name">var <span class="ident">X</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.LinearRegression.importance"><code class="name">var <span class="ident">importance</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.LinearRegression.w"><code class="name">var <span class="ident">w</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.LinearRegression.y"><code class="name">var <span class="ident">y</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.LinearRegression.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, dataset: numpy.ndarray, values: numpy.ndarray, verbose: bool = False) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Performs linear multiple model training</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the scaled data.</dd>
<dt><strong><code>values</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A set of values returned by the linear function.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>importance</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the importance of each feature.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, dataset: ndarray, values: ndarray, verbose: bool = False) -&gt; None:
    &#34;&#34;&#34;Performs linear multiple model training

    Parameters
    ----------
    dataset : `np.array`
        An array containing the scaled data.
    values : `np.ndarray`
        A set of values returned by the linear function.

    Returns
    -------
    importance : `np.array`
        An array containing the importance of each feature.

    &#34;&#34;&#34;

    self.X = dataset
    self.y = values

    U, S, VT = np.linalg.svd(self.X, full_matrices=False)
    self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ self.y

    for i in range(self.X.shape[0]):
        a = np.around(self.w[i], decimals=8)
        self.importance.append(a)

    if verbose:
        print(&#34;\nSummary:&#34;)
        print(&#34;--------&#34;)
        print(&#34;\nParameters:&#34;, np.array(self.importance).shape)
        print(&#34;RMSE: {:.4f}&#34;.format(mean_square_error(self.y, self.predict(self.X))))</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.LinearRegression.get_importances"><code class="name flex">
<span>def <span class="ident">get_importances</span></span>(<span>self, print_important_features: bool = False) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the important features</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>print_important_features</code></strong> :&ensp;<code>bool</code></dt>
<dd>determines whether or not are printed on the screen. By default it is set to <code>False</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>importance</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the importance of each feature.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_importances(self, print_important_features: bool = False) -&gt; ndarray:
    &#34;&#34;&#34;
    Returns the important features

    Parameters
    ----------
    print_important_features : `bool`
        determines whether or not are printed on the screen. By default it is set to `False`.

    Returns
    -------
    importance : `np.array`
        An array containing the importance of each feature.


    &#34;&#34;&#34;
    if print_important_features:
        for i, a in enumerate(self.importance):
            print(f&#34;The importance of the {i+1} feature is {a}&#34;)
    return np.array(self.importance)</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.LinearRegression.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, datapoints: numpy.ndarray) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Performs predictions for a set of points</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>datapoints</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the values of the independent variable.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, datapoints: ndarray) -&gt; ndarray:
    &#34;&#34;&#34;
    Performs predictions for a set of points

    Parameters
    ----------
    datapoints : `np.array`
        An array containing the values of the independent variable.

    &#34;&#34;&#34;
    return np.array(self.importance) @ datapoints</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression"><code class="flex name class">
<span>class <span class="ident">LogisticRegression</span></span>
</code></dt>
<dd>
<div class="desc"><p>class implementing multiple logistic regression</p>
<p>The class initializer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LogisticRegression:
    &#34;&#34;&#34;class implementing multiple logistic regression&#34;&#34;&#34;

    __slots__ = [&#34;importance&#34;, &#34;X&#34;, &#34;y&#34;, &#34;w&#34;]

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;The class initializer&#34;&#34;&#34;

        self.importance = []

    def fit(self, dataset: ndarray, values: ndarray) -&gt; None:
        &#34;&#34;&#34;Performs linear multiple model training

        Parameters
        ----------
        dataset : `np.array`
            An array containing the scaled data.
        values : `np.ndarray`
            A set of values returned by the linear function.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.

        &#34;&#34;&#34;

        self.X = dataset
        self.y = values

        U, S, VT = np.linalg.svd(self.X, full_matrices=False)

        inverse_sig = np.vectorize(sigmoide_inv)
        self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ inverse_sig(self.y)

        if self.y.shape[1] &gt; 1:
            for row in self.w:
                self.importance.append(np.around(np.max(row), decimals=8))
        else:
            for i in range(self.X.shape[0]):
                a = np.around(self.w[i], decimals=8)
                self.importance.append(a)

    def predict(self, datapoints: ndarray) -&gt; ndarray:
        &#34;&#34;&#34;
        Performs predictions for a set of points

        Parameters
        ----------
        datapoints : `np.array`
            An array containing the values of the independent variable.

        &#34;&#34;&#34;
        sig = np.vectorize(sigmoide)

        return sig(np.array(self.importance) @ datapoints)

    def get_importances(self, print_important_features: bool = False) -&gt; ndarray:
        &#34;&#34;&#34;
        Returns the important features

        Parameters
        ----------
        print_important_features : `bool`
            determines whether or not are printed on the screen. By default it is set to `False`.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.


        &#34;&#34;&#34;
        if print_important_features:
            for i, a in enumerate(self.importance):
                print(f&#34;The importance of the {i+1} feature is {a}&#34;)
        return np.array(self.importance)</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.LogisticRegression.X"><code class="name">var <span class="ident">X</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression.importance"><code class="name">var <span class="ident">importance</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression.w"><code class="name">var <span class="ident">w</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression.y"><code class="name">var <span class="ident">y</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.LogisticRegression.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, dataset: numpy.ndarray, values: numpy.ndarray) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Performs linear multiple model training</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the scaled data.</dd>
<dt><strong><code>values</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A set of values returned by the linear function.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>importance</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the importance of each feature.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, dataset: ndarray, values: ndarray) -&gt; None:
    &#34;&#34;&#34;Performs linear multiple model training

    Parameters
    ----------
    dataset : `np.array`
        An array containing the scaled data.
    values : `np.ndarray`
        A set of values returned by the linear function.

    Returns
    -------
    importance : `np.array`
        An array containing the importance of each feature.

    &#34;&#34;&#34;

    self.X = dataset
    self.y = values

    U, S, VT = np.linalg.svd(self.X, full_matrices=False)

    inverse_sig = np.vectorize(sigmoide_inv)
    self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ inverse_sig(self.y)

    if self.y.shape[1] &gt; 1:
        for row in self.w:
            self.importance.append(np.around(np.max(row), decimals=8))
    else:
        for i in range(self.X.shape[0]):
            a = np.around(self.w[i], decimals=8)
            self.importance.append(a)</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression.get_importances"><code class="name flex">
<span>def <span class="ident">get_importances</span></span>(<span>self, print_important_features: bool = False) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the important features</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>print_important_features</code></strong> :&ensp;<code>bool</code></dt>
<dd>determines whether or not are printed on the screen. By default it is set to <code>False</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>importance</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the importance of each feature.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_importances(self, print_important_features: bool = False) -&gt; ndarray:
    &#34;&#34;&#34;
    Returns the important features

    Parameters
    ----------
    print_important_features : `bool`
        determines whether or not are printed on the screen. By default it is set to `False`.

    Returns
    -------
    importance : `np.array`
        An array containing the importance of each feature.


    &#34;&#34;&#34;
    if print_important_features:
        for i, a in enumerate(self.importance):
            print(f&#34;The importance of the {i+1} feature is {a}&#34;)
    return np.array(self.importance)</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, datapoints: numpy.ndarray) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Performs predictions for a set of points</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>datapoints</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the values of the independent variable.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, datapoints: ndarray) -&gt; ndarray:
    &#34;&#34;&#34;
    Performs predictions for a set of points

    Parameters
    ----------
    datapoints : `np.array`
        An array containing the values of the independent variable.

    &#34;&#34;&#34;
    sig = np.vectorize(sigmoide)

    return sig(np.array(self.importance) @ datapoints)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.OneHotEncoder"><code class="flex name class">
<span>class <span class="ident">OneHotEncoder</span></span>
</code></dt>
<dd>
<div class="desc"><p>Class used to encode categorical variables.
It receives an array of integers and returns a binary array using the one-hot encoding method.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OneHotEncoder:
    &#34;&#34;&#34;
    Class used to encode categorical variables.
    It receives an array of integers and returns a binary array using the one-hot encoding method.
    &#34;&#34;&#34;

    __slots__ = [&#34;x&#34;]

    def __init__(self) -&gt; None:
        pass

    def encode(self, x: ndarray | list):
        self.x = x

        if not isinstance(self.x, ndarray):
            self.x = np.array(self.x)  # If not numpy array then convert it

        y = np.zeros(
            (self.x.size, self.x.max() + 1)
        )  # Build matrix of (size num of entries) x (max value + 1)

        y[np.arange(self.x.size), self.x] = 1  # Label with ones

        return y

    def decode(self, x: ndarray | list) -&gt; ndarray:
        if not isinstance(x, ndarray):
            x = np.array(x)  # If not numpy array then convert it

        # Regresamos los valores max de cada renglon
        y = np.argmax(x, axis=1)

        return y</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.OneHotEncoder.x"><code class="name">var <span class="ident">x</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.OneHotEncoder.decode"><code class="name flex">
<span>def <span class="ident">decode</span></span>(<span>self, x: numpy.ndarray | list) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decode(self, x: ndarray | list) -&gt; ndarray:
    if not isinstance(x, ndarray):
        x = np.array(x)  # If not numpy array then convert it

    # Regresamos los valores max de cada renglon
    y = np.argmax(x, axis=1)

    return y</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.OneHotEncoder.encode"><code class="name flex">
<span>def <span class="ident">encode</span></span>(<span>self, x: numpy.ndarray | list)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode(self, x: ndarray | list):
    self.x = x

    if not isinstance(self.x, ndarray):
        self.x = np.array(self.x)  # If not numpy array then convert it

    y = np.zeros(
        (self.x.size, self.x.max() + 1)
    )  # Build matrix of (size num of entries) x (max value + 1)

    y[np.arange(self.x.size), self.x] = 1  # Label with ones

    return y</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.PerformanceMeasures"><code class="flex name class">
<span>class <span class="ident">PerformanceMeasures</span></span>
</code></dt>
<dd>
<div class="desc"><p>Class with methods to measure performance</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PerformanceMeasures:
    &#34;&#34;&#34;Class with methods to measure performance&#34;&#34;&#34;

    def __init__(self) -&gt; None:
        pass

    # Performance measure Res_T
    def f_mean(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; None:
        n = len(labels)

        F_vec = self._f1_score(y_true, y_pred, labels=labels)
        a = np.sum(F_vec)

        for i in range(len(F_vec)):
            print(&#34;F-measure of label &#34;, labels[i], &#34; -&gt; &#34;, F_vec[i])

        print(&#34;Mean of F-measure -&gt; &#34;, a / n)

    # Performance measure Res_P
    def resp(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; None:
        # We initialize sum counters
        sum1 = 0
        sum2 = 0

        # Calculamos T_C
        T_C = len(y_true)
        for i in range(len(labels)):
            # We calculate instances of the classes and their F-measures
            sum1 += (1 - ((y_true == labels[i]).sum() / T_C)) * self._fi_measure(
                y_true, y_pred, labels, i
            )
            sum2 += 1 - ((y_true == labels[i]).sum()) / T_C

        # Print the metric corresponding to the prediction vector
        print(&#34;Metric Res_p -&gt;&#34;, sum1 / sum2)

    def _fi_measure(self, y_true: ndarray, y_pred: ndarray, labels: list, i: int) -&gt; int:
        F_vec = self._f1_score(y_true, y_pred, labels=labels)

        return F_vec[i]  # We return the position of the f1-score corresponding to the label

    # Summary of the labels predicted
    def _summary_pred(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; None:
        count_mat = self._confu_mat(y_true, y_pred, labels)
        print(&#34;        &#34;, end=&#34;&#34;)
        for i in range(len(labels)):
            print(&#34;|--&#34;, labels[i], &#34;--&#34;, end=&#34;&#34;)
            if i + 1 == len(labels):
                print(&#34;|&#34;, end=&#34;&#34;)
        for i in range(len(labels)):
            print(&#34;&#34;)
            print(&#34;|--&#34;, labels[i], &#34;--|&#34;, end=&#34;&#34;)
            for j in range(len(labels)):
                if j != 0:
                    print(&#34; &#34;, end=&#34;&#34;)
                print(&#34;  &#34;, int(count_mat[i, j]), &#34;  &#34;, end=&#34;&#34;)

    def _f1_score(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; ndarray:
        f1_vec = np.zeros(len(labels))

        # Calculate confusion mat
        count_mat = self._confu_mat(y_true, y_pred, labels)

        # sums over columns
        sum1 = np.sum(count_mat, axis=0)
        # sums over rows
        sum2 = np.sum(count_mat, axis=1)
        # Iterate over labels to calculate f1 scores of each one
        for i in range(len(labels)):
            precision = count_mat[i, i] / (sum1[i])
            recall = count_mat[i, i] / (sum2[i])

            f1_vec[i] = 2 * ((precision * recall) / (precision + recall))

        return f1_vec

    # Returns confusion matrix of predictions
    def _confu_mat(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; ndarray:
        labels = np.array(labels)
        count_mat = np.zeros((len(labels), len(labels)))

        for i in range(len(labels)):
            for j in range(len(y_pred)):
                if y_pred[j] == labels[i]:
                    if y_pred[j] == y_true[j]:
                        count_mat[i, i] += 1
                    else:
                        x = np.where(labels == y_true[j])
                        count_mat[i, x[0]] += 1

        return count_mat</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.PerformanceMeasures.f_mean"><code class="name flex">
<span>def <span class="ident">f_mean</span></span>(<span>self, y_true: numpy.ndarray, y_pred: numpy.ndarray, labels: list) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def f_mean(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; None:
    n = len(labels)

    F_vec = self._f1_score(y_true, y_pred, labels=labels)
    a = np.sum(F_vec)

    for i in range(len(F_vec)):
        print(&#34;F-measure of label &#34;, labels[i], &#34; -&gt; &#34;, F_vec[i])

    print(&#34;Mean of F-measure -&gt; &#34;, a / n)</code></pre>
</details>
</dd>
<dt id="likelihood.tools.tools.PerformanceMeasures.resp"><code class="name flex">
<span>def <span class="ident">resp</span></span>(<span>self, y_true: numpy.ndarray, y_pred: numpy.ndarray, labels: list) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resp(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; None:
    # We initialize sum counters
    sum1 = 0
    sum2 = 0

    # Calculamos T_C
    T_C = len(y_true)
    for i in range(len(labels)):
        # We calculate instances of the classes and their F-measures
        sum1 += (1 - ((y_true == labels[i]).sum() / T_C)) * self._fi_measure(
            y_true, y_pred, labels, i
        )
        sum2 += 1 - ((y_true == labels[i]).sum()) / T_C

    # Print the metric corresponding to the prediction vector
    print(&#34;Metric Res_p -&gt;&#34;, sum1 / sum2)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.autocorr"><code class="flex name class">
<span>class <span class="ident">autocorr</span></span>
<span>(</span><span>x: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the autocorrelation of the data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>z</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the autocorrelation of the data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class autocorr:
    &#34;&#34;&#34;Calculates the autocorrelation of the data

    Parameters
    ----------
    x : `np.array`
        An array containing the data.

    Returns
    -------
    z : `np.array`
        An array containing the autocorrelation of the data.

    &#34;&#34;&#34;

    __slots__ = [&#34;x&#34;, &#34;result&#34;, &#34;z&#34;]

    def __init__(self, x: ndarray):
        self.x = x
        self.result = np.correlate(x, x, mode=&#34;full&#34;)
        self.z = self.result[self.result.size // 2 :]
        self.z = self.z / float(np.abs(self.z).max())

    def plot(self):
        plt.plot(range(len(self.z)), self.z, label=&#34;Autocorrelation&#34;)
        plt.legend()
        plt.show()

    def __call__(self):
        return self.z</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.autocorr.result"><code class="name">var <span class="ident">result</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.autocorr.x"><code class="name">var <span class="ident">x</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.autocorr.z"><code class="name">var <span class="ident">z</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.autocorr.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(self):
    plt.plot(range(len(self.z)), self.z, label=&#34;Autocorrelation&#34;)
    plt.legend()
    plt.show()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.corr"><code class="flex name class">
<span>class <span class="ident">corr</span></span>
<span>(</span><span>x: numpy.ndarray, y: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the correlation of the data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>z</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the correlation of <code>x</code> and <code>y</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class corr:
    &#34;&#34;&#34;Calculates the correlation of the data

    Parameters
    ----------
    x : `np.array`
        An array containing the data.
    y : `np.array`
        An array containing the data.

    Returns
    -------
    z : `np.array`
        An array containing the correlation of `x` and `y`.

    &#34;&#34;&#34;

    __slots__ = [&#34;x&#34;, &#34;y&#34;, &#34;result&#34;, &#34;z&#34;]

    def __init__(self, x: ndarray, y: ndarray):
        self.x = x
        self.y = y
        self.result = np.correlate(x, y, mode=&#34;full&#34;)
        self.z = self.result[self.result.size // 2 :]
        self.z = self.z / float(np.abs(self.z).max())

    def plot(self):
        plt.plot(range(len(self.z)), self.z, label=&#34;Correlation&#34;)
        plt.legend()
        plt.show()

    def __call__(self):
        return self.z</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.corr.result"><code class="name">var <span class="ident">result</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.corr.x"><code class="name">var <span class="ident">x</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.corr.y"><code class="name">var <span class="ident">y</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="likelihood.tools.tools.corr.z"><code class="name">var <span class="ident">z</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.corr.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(self):
    plt.plot(range(len(self.z)), self.z, label=&#34;Correlation&#34;)
    plt.legend()
    plt.show()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="likelihood.tools" href="index.html">likelihood.tools</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="likelihood.tools.tools.cal_average" href="#likelihood.tools.tools.cal_average">cal_average</a></code></li>
<li><code><a title="likelihood.tools.tools.cal_missing_values" href="#likelihood.tools.tools.cal_missing_values">cal_missing_values</a></code></li>
<li><code><a title="likelihood.tools.tools.calculate_probability" href="#likelihood.tools.tools.calculate_probability">calculate_probability</a></code></li>
<li><code><a title="likelihood.tools.tools.cdf" href="#likelihood.tools.tools.cdf">cdf</a></code></li>
<li><code><a title="likelihood.tools.tools.check_nan_inf" href="#likelihood.tools.tools.check_nan_inf">check_nan_inf</a></code></li>
<li><code><a title="likelihood.tools.tools.difference_quotient" href="#likelihood.tools.tools.difference_quotient">difference_quotient</a></code></li>
<li><code><a title="likelihood.tools.tools.estimate_gradient" href="#likelihood.tools.tools.estimate_gradient">estimate_gradient</a></code></li>
<li><code><a title="likelihood.tools.tools.fft_denoise" href="#likelihood.tools.tools.fft_denoise">fft_denoise</a></code></li>
<li><code><a title="likelihood.tools.tools.generate_feature_yaml" href="#likelihood.tools.tools.generate_feature_yaml">generate_feature_yaml</a></code></li>
<li><code><a title="likelihood.tools.tools.generate_series" href="#likelihood.tools.tools.generate_series">generate_series</a></code></li>
<li><code><a title="likelihood.tools.tools.get_period" href="#likelihood.tools.tools.get_period">get_period</a></code></li>
<li><code><a title="likelihood.tools.tools.mean_square_error" href="#likelihood.tools.tools.mean_square_error">mean_square_error</a></code></li>
<li><code><a title="likelihood.tools.tools.minibatches" href="#likelihood.tools.tools.minibatches">minibatches</a></code></li>
<li><code><a title="likelihood.tools.tools.partial_difference_quotient" href="#likelihood.tools.tools.partial_difference_quotient">partial_difference_quotient</a></code></li>
<li><code><a title="likelihood.tools.tools.sigmoide" href="#likelihood.tools.tools.sigmoide">sigmoide</a></code></li>
<li><code><a title="likelihood.tools.tools.sigmoide_inv" href="#likelihood.tools.tools.sigmoide_inv">sigmoide_inv</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="likelihood.tools.tools.DataFrameEncoder" href="#likelihood.tools.tools.DataFrameEncoder">DataFrameEncoder</a></code></h4>
<ul class="two-column">
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.decode" href="#likelihood.tools.tools.DataFrameEncoder.decode">decode</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.decoding_list" href="#likelihood.tools.tools.DataFrameEncoder.decoding_list">decoding_list</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.encode" href="#likelihood.tools.tools.DataFrameEncoder.encode">encode</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.encoding_list" href="#likelihood.tools.tools.DataFrameEncoder.encoding_list">encoding_list</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.get_dictionaries" href="#likelihood.tools.tools.DataFrameEncoder.get_dictionaries">get_dictionaries</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.load_config" href="#likelihood.tools.tools.DataFrameEncoder.load_config">load_config</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.train" href="#likelihood.tools.tools.DataFrameEncoder.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.DataScaler" href="#likelihood.tools.tools.DataScaler">DataScaler</a></code></h4>
<ul class="two-column">
<li><code><a title="likelihood.tools.tools.DataScaler.data_scaled" href="#likelihood.tools.tools.DataScaler.data_scaled">data_scaled</a></code></li>
<li><code><a title="likelihood.tools.tools.DataScaler.dataset_" href="#likelihood.tools.tools.DataScaler.dataset_">dataset_</a></code></li>
<li><code><a title="likelihood.tools.tools.DataScaler.rescale" href="#likelihood.tools.tools.DataScaler.rescale">rescale</a></code></li>
<li><code><a title="likelihood.tools.tools.DataScaler.scale" href="#likelihood.tools.tools.DataScaler.scale">scale</a></code></li>
<li><code><a title="likelihood.tools.tools.DataScaler.transpose" href="#likelihood.tools.tools.DataScaler.transpose">transpose</a></code></li>
<li><code><a title="likelihood.tools.tools.DataScaler.values" href="#likelihood.tools.tools.DataScaler.values">values</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.FeatureSelection" href="#likelihood.tools.tools.FeatureSelection">FeatureSelection</a></code></h4>
<ul class="">
<li><code><a title="likelihood.tools.tools.FeatureSelection.X" href="#likelihood.tools.tools.FeatureSelection.X">X</a></code></li>
<li><code><a title="likelihood.tools.tools.FeatureSelection.all_features_imp_graph" href="#likelihood.tools.tools.FeatureSelection.all_features_imp_graph">all_features_imp_graph</a></code></li>
<li><code><a title="likelihood.tools.tools.FeatureSelection.get_digraph" href="#likelihood.tools.tools.FeatureSelection.get_digraph">get_digraph</a></code></li>
<li><code><a title="likelihood.tools.tools.FeatureSelection.not_features" href="#likelihood.tools.tools.FeatureSelection.not_features">not_features</a></code></li>
<li><code><a title="likelihood.tools.tools.FeatureSelection.scaler" href="#likelihood.tools.tools.FeatureSelection.scaler">scaler</a></code></li>
<li><code><a title="likelihood.tools.tools.FeatureSelection.w_dict" href="#likelihood.tools.tools.FeatureSelection.w_dict">w_dict</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.LinearRegression" href="#likelihood.tools.tools.LinearRegression">LinearRegression</a></code></h4>
<ul class="two-column">
<li><code><a title="likelihood.tools.tools.LinearRegression.X" href="#likelihood.tools.tools.LinearRegression.X">X</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.fit" href="#likelihood.tools.tools.LinearRegression.fit">fit</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.get_importances" href="#likelihood.tools.tools.LinearRegression.get_importances">get_importances</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.importance" href="#likelihood.tools.tools.LinearRegression.importance">importance</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.predict" href="#likelihood.tools.tools.LinearRegression.predict">predict</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.w" href="#likelihood.tools.tools.LinearRegression.w">w</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.y" href="#likelihood.tools.tools.LinearRegression.y">y</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.LogisticRegression" href="#likelihood.tools.tools.LogisticRegression">LogisticRegression</a></code></h4>
<ul class="two-column">
<li><code><a title="likelihood.tools.tools.LogisticRegression.X" href="#likelihood.tools.tools.LogisticRegression.X">X</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.fit" href="#likelihood.tools.tools.LogisticRegression.fit">fit</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.get_importances" href="#likelihood.tools.tools.LogisticRegression.get_importances">get_importances</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.importance" href="#likelihood.tools.tools.LogisticRegression.importance">importance</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.predict" href="#likelihood.tools.tools.LogisticRegression.predict">predict</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.w" href="#likelihood.tools.tools.LogisticRegression.w">w</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.y" href="#likelihood.tools.tools.LogisticRegression.y">y</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.OneHotEncoder" href="#likelihood.tools.tools.OneHotEncoder">OneHotEncoder</a></code></h4>
<ul class="">
<li><code><a title="likelihood.tools.tools.OneHotEncoder.decode" href="#likelihood.tools.tools.OneHotEncoder.decode">decode</a></code></li>
<li><code><a title="likelihood.tools.tools.OneHotEncoder.encode" href="#likelihood.tools.tools.OneHotEncoder.encode">encode</a></code></li>
<li><code><a title="likelihood.tools.tools.OneHotEncoder.x" href="#likelihood.tools.tools.OneHotEncoder.x">x</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.PerformanceMeasures" href="#likelihood.tools.tools.PerformanceMeasures">PerformanceMeasures</a></code></h4>
<ul class="">
<li><code><a title="likelihood.tools.tools.PerformanceMeasures.f_mean" href="#likelihood.tools.tools.PerformanceMeasures.f_mean">f_mean</a></code></li>
<li><code><a title="likelihood.tools.tools.PerformanceMeasures.resp" href="#likelihood.tools.tools.PerformanceMeasures.resp">resp</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.autocorr" href="#likelihood.tools.tools.autocorr">autocorr</a></code></h4>
<ul class="">
<li><code><a title="likelihood.tools.tools.autocorr.plot" href="#likelihood.tools.tools.autocorr.plot">plot</a></code></li>
<li><code><a title="likelihood.tools.tools.autocorr.result" href="#likelihood.tools.tools.autocorr.result">result</a></code></li>
<li><code><a title="likelihood.tools.tools.autocorr.x" href="#likelihood.tools.tools.autocorr.x">x</a></code></li>
<li><code><a title="likelihood.tools.tools.autocorr.z" href="#likelihood.tools.tools.autocorr.z">z</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.corr" href="#likelihood.tools.tools.corr">corr</a></code></h4>
<ul class="">
<li><code><a title="likelihood.tools.tools.corr.plot" href="#likelihood.tools.tools.corr.plot">plot</a></code></li>
<li><code><a title="likelihood.tools.tools.corr.result" href="#likelihood.tools.tools.corr.result">result</a></code></li>
<li><code><a title="likelihood.tools.tools.corr.x" href="#likelihood.tools.tools.corr.x">x</a></code></li>
<li><code><a title="likelihood.tools.tools.corr.y" href="#likelihood.tools.tools.corr.y">y</a></code></li>
<li><code><a title="likelihood.tools.tools.corr.z" href="#likelihood.tools.tools.corr.z">z</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>